[
  {
    "objectID": "1_mortgage.html",
    "href": "1_mortgage.html",
    "title": "Mortgage Simulation",
    "section": "",
    "text": "The algorithm\nThis example will replicate the core idea of the simulation.\nWe will do 99 quantile regressions for each of the 3 banks data-sets. Then, relying of the fact that the data-sets are unified, for each observation we will predict the price over 297 regression models, 99 for each bank. This will allow a reconstruction of a predicted price distribution for each loan in each of the 3 banks.\nOnce we have the price distributions for each loan two types of comparisons are possible:\n\nWe assume that a consumer’s quantile is predetermined. that is, the researcher has some unobservables which don’t exist in the data but the customer and the banks know the values of these unobservables. In this case, if a customer belongs to the 42nd quantile, than we should compare the 42nd quantile between the 3 banks. To measure the expected savings from bidding between banks sums up to taking the minimum from each quantile of the 3 price distributions.\nMore interesting to show is the case where the customer’s quantile is not determined but random. Conditional on the customer and transaction’s characteristics she could be in quantile 42 in bank A, quantile 14 in bank B and quantile 75 in bank C. To simulate this process we need to draw a triplet of quantiles, one for each bank. If we draw enough triplets we can re-construct the predicted price distribution for this customer if she had bid only on bank, two banks or all three of them by taking the minimum price from each combination of the quantile triplet.\n\n\n\nLoad packages and data\n\nlibrary(tidyverse)\noptions(scipen = 999, digits = 3)\n\n# data  bases\nload('mortgage_data/data_1_raw_database_v2.Rdata')\n\nWe have 3 data frames, one for each bank, each has 50 columns. The data sets have been unified so all explanatory variables have the same scale and meaning. So, when we take an observation of a loan purchased from bank A and predict its price on model trained with another bank’s data, the results make sense.\n\nls()\n\n[1] \"df_bank_A\" \"df_bank_B\" \"df_bank_C\"\n\nmap(ls(), ~ dim(get(.)))\n\n[[1]]\n[1] 25779    50\n\n[[2]]\n[1] 26958    50\n\n[[3]]\n[1] 44944    50\n\n\n\n\nRegression formula\nNext, an example of regression formula. In the research, we has nearly 50 explanatory variables including many categorical variables such as fixed effects for time, asset types and the loan portfolio. To run this big quantile regression one needs large data-set, otherwise the regression model will not converge, especially in the corner quantliles - for instance quantiles below 5 and above 95 may not converge. One solution would be to give up on the corner quantiles and to be satisfied with incomplete distribution. Another solution is to simply the regression formula. A third options could by to use some kind of Lasso Penalized Quantile Regression.\n\nreg_formula <-\nformula(\"interest ~ Loan_to_Value + Purchase_Purpose +\n  service_commission + asset_type + \n  log_loan_size + log_asset_value + log_disposable_income + \n  demographics +\n  porfolio_fixed_effects + \n  amortization_periods +\n  time_fixed_effect\")\n\n\n\nQuantile regressions\n\ntaus <- 1:99/100\nc(head(taus), \"....\", tail(taus))\n\n [1] \"0.01\" \"0.02\" \"0.03\" \"0.04\" \"0.05\" \"0.06\" \"....\" \"0.94\" \"0.95\" \"0.96\"\n[11] \"0.97\" \"0.98\" \"0.99\"\n\n\nRunning 297 quantile regression is an Embarrassingly parallel computing problem. Here we use the package furrr that implements parallel computing to the purrr map functions with future supported back-end. The change in syntax is just write future_map() instead of map() . The function quantreg::rq computes and creates the quantile regression model.\n\n\nlibrary(quantreg) # a quantile regression package authord by Roger Koenker himself\nlibrary(furrr)\n\nfuture::plan(multiprocess) # parallel processing\n\nmodels_A <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_A, method = 'pfn' ), .progress = T )\n\nmodels_B <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_B, method = 'pfn' ), .progress = T)\n\nmodels_C <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_C, method = 'pfn' ), .progress = T)\n\nfuture:::ClusterRegistry(\"stop\") # close workers. \n\nFor each bank we get 99 quantile regressions packed in a list object.\n\n\n\nPredictions\nIn research, we computed the simulation for each loan in the data. Then, we reported summary statistics about the possible savings consumers could have had if they could and knew how to ask for bids from more banks. here, a small data-set will suffice.\n\nload('mortgage_data/data_3_for_predictions.RData')\n\nFor each bank we run the predictions separately.\nThe next function computes the predictions for each list of models.\n\n# create predictions\nf_predict <- function(.models , .data, .bank){\n  # run pfedictions for each of the 9 models\n  x1 <- map_dfc(.models, ~ predict(.x, .data)) %>% rownames_to_column(var = \"id\") \n  \n  # arrange the data into long format to use with ggplot\n  x2 <- x1 %>% gather( key = \"quantile\", value = \"prediction\", 2:ncol(x))\n  x2 <- x2 %>% mutate(quantile = as.numeric(str_extract(quantile, \"\\\\d+\")),\n                      bank = .bank)\n  x2\n}\n\np_bank_A <- f_predict(models_A, df_for_predictions, \"A\")\np_bank_B <- f_predict(models_B, df_for_predictions, \"B\")\np_bank_C <- f_predict(models_C, df_for_predictions, \"C\")\n\n# bind  all predictions\np_banks <- bind_rows(p_bank_A, p_bank_B, p_bank_C) %>% mutate(id = as.numeric(id))\n\nLets see what we got:\n\nhead(p_banks %>% arrange(id))\n\n# A tibble: 6 x 4\n     id quantile prediction bank \n  <dbl>    <dbl>      <dbl> <chr>\n1     1        1       2.45 A    \n2     1        2       2.55 A    \n3     1        3       2.57 A    \n4     1        4       2.60 A    \n5     1        5       2.59 A    \n6     1        6       2.58 A    \n\n\nFor each loan we get 297 rows with each predicted price.\nLets look at the price distribution of a loan:\n\np_banks %>% filter(id == 10) %>%\n  ggplot(aes(x = quantile, y = prediction, color = bank)) +\n  geom_line()\n\n\n\n\nIts interesting to see that the price variance in bank B is very large. If this customer would be in the lower percentiles he is better of at bank B, but as the percentiles increase he is better of at bank A. the the end of the percentile range, the lowest price is in bank C.\n\n\nSimulation\nAs explained above in the algorithm section, its more interesting to assume that quentiles are not determined but random. The next function draws random triplets of values \\(\\in [0, 99]\\) for chosen quantiles and matched them to the corresponding prices. Then, for each possible combination of those price values, the minimum price is chosen. For a given loan, 7 price distributions are constructed. 3 when she goes to only one bank - for each of the 3 banks, 3 more when she bids 2 banks: AB, AC, BC, and lastly when she bids all banks.\n\nf_simulation_dist_different_draw <- function(id_number, p_data, n = 1000) {\n  # draw random quantiles for each bank\n  r1 <- data.frame(\n    simulation_n = rep(1:n, 3),\n    id = id_number,\n    bank = c(rep(\"A\", n), rep(\"B\", n), rep(\"C\", n)), \n    quantile = round(runif(3 * n, 0.01, 0.99) * 100), stringsAsFactors = F )\n    \n # join predictions from the p_bank Rata \n  r2 <- left_join(r1, p_data ,by = c(\"id\", \"bank\", \"quantile\") )\n  \n  # For each draw triplet we choose all combinations of 2 banks,\n  # As if the customer got bids from two banks.\n  # There are 3 options: A-B, A-C, B-C\n  r3a <- r2 %>%  select(- quantile)\n  \n  # option 1\n  r3b1 <- r2 %>% filter(bank != \"C\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"A_B\")\n  \n  # option 2\n  r3b2 <- r2 %>% filter(bank != \"B\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"A_C\")\n  \n  # option 3\n  r3b3 <- r2 %>% filter(bank != \"C\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"B_C\")\n  \n  \n  # 1 option for bidding in all three banks\n  r3c <- r2 %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"all_3_banks\")\n  \n  # bind everything together\n  r4 <- bind_rows(r3a,r3b1, r3b2, r3b3, r3c)\n  \n  # a Dummy variable to choose from which bank the customer is\n  r4 <- r4 %>%\n    mutate(A = if_else( bank %in% c(\"A\", \"A_B\", \"A_C\", \"all_3_banks\"), 1, 0),\n           B = if_else( bank %in% c(\"B\", \"A_B\", \"B_C\", \"all_3_banks\"), 1, 0),\n           C = if_else( bank %in% c(\"C\", \"A_C\", \"B_C\", \"all_3_banks\"), 1, 0))\n  \n  r4\n}\n\nLets simulate the price distributions for loan 10:\n\ncustomer_10 <- f_simulation_dist_different_draw(id_number = 10,  p_banks)\n\nWe got a data ready to plot:\n\nhead(customer_10)\n\n  simulation_n id bank prediction A B C\n1            1 10    A       3.17 1 0 0\n2            2 10    A       3.73 1 0 0\n3            3 10    A       3.72 1 0 0\n4            4 10    A       3.77 1 0 0\n5            5 10    A       3.63 1 0 0\n6            6 10    A       3.16 1 0 0\n\n\nIt’s not easy to look at 7 distributions simultaneously as well as in this case it doesn’t really make sense. If the customer originally was from bank C (where she runs her checking account and inclined to ask for a mortgage there), than we would like to see what she could have gained from an additional bid from bank A, B or both.\nThe f_plot function plots 4 distributions from the perspective of a certain bank’s customer:\n\nf_plot <- function(data, bank_choice){\n  data %>% filter(.data[[bank_choice]] == 1)  %>%\n    ggplot(aes(x = prediction, color = bank, fill = bank)) +\n    geom_density(alpha = 0.1) \n}\n\n\nf_plot(customer_10, 'C')\n\n\n\n\nWe can also look at it from every angle\n\np4 <- f_plot(customer_10, \"A\") + coord_cartesian(xlim = c(2.2, 5))\np5 <- f_plot(customer_10, \"B\")+ coord_cartesian(xlim = c(2.2, 5))\np6 <- f_plot(customer_10, \"C\")+ coord_cartesian(xlim = c(2.2, 5))\n\nlibrary(patchwork)\np4 / p5 / p6\n\n\n\n\nLets take it with a boxplot:\n\nggplot(customer_10,\n       aes(x = fct_reorder(bank, prediction, .fun = median),y = prediction)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(3, 4.5)) +\n  labs(x = \"Bank combination bids taken from\",\n       y = \"Predicted price\")"
  },
  {
    "objectID": "2_merger_sim_intro.html",
    "href": "2_merger_sim_intro.html",
    "title": "What is Merger Simulation?",
    "section": "",
    "text": "The next pages presents examples of horizontal merger simulation implemented in R code.\nMerger simulation is a quantitative tool to create prediction about the probable outcome effect of a merger on competition. The process can be divided to three parts:\n(1) Selection of a structural model of supply and demand.\n(2) Calibration model’s parameters.\n(3) Computation of new equilibrium that will prevail post merger.\nNext, I’ll briefly describe the selected three parts chosen for the forthcoming examples. Afterwards, a brief view of the equation system that emerge from the model, and finally, a few references for those who’d like to deepen their knowledge in the subject.\n(1) The structural model\nOn the supply side we’ll use a Bertrand model with differentiated products. On the demand side versions of logit demand systems will be used. This specification is common for competition agencies to work with.\n(2) model calibration\nTwo techniques are common, the harder one is implementing demand estimation of the demand function parameters. It is a demanding process both in terms of the required data and in terms of the required scope of work. Another approach is to calibrate the model using carefully selected parameters - this is what we’ll do here.\n(3) Equilibrium computation\nIn the case of a non-linear demand system such as we have here, no analytic solution exists. Some kind of numeric method is required to solve the equation system - here I’ll use fixed point iteration.\nNext, we explain a little bit about the chosen structural model. here is a link to skip the mathmatical model and go to the first example of multinomial logit"
  },
  {
    "objectID": "2_merger_sim_intro.html#bertrand-model-with-differentiated-products",
    "href": "2_merger_sim_intro.html#bertrand-model-with-differentiated-products",
    "title": "What is Merger Simulation?",
    "section": "Bertrand model with differentiated products",
    "text": "Bertrand model with differentiated products\nA very good explanation of how the betrand model works is written by Jonas Björnerstedt (Swedish competition authority) and Frank Verboven (University of Leuven) in their merger simulation implementation in stata. The next few images are taken from section 2 of their paper:"
  },
  {
    "objectID": "2_merger_sim_intro.html#the-multinomial-logit-model",
    "href": "2_merger_sim_intro.html#the-multinomial-logit-model",
    "title": "What is Merger Simulation?",
    "section": "The multinomial logit model",
    "text": "The multinomial logit model\nA good explanation of the logit model is in the vignette of the r package antitrust by authored by Charles Taragin and Michael Sandfort. The explanation below is a selection from the package’s vignette:\nLogit demand is based on a discrete choice model that assumes that each consumer is willing to purchase at most a single unit of one product from the \\(n\\) products available in the market. The assumptions underlying Logit demand imply that the probability that a consumer purchases product \\(i \\in n\\) is given by\n\\[\\begin{align*}\n  s_i=& \\frac{\\exp(V_i)}{\\sum\\limits_{k \\in n}\\exp(V_k)},&\n\\end{align*}\\]\nwhere \\(s_i\\) is product \\(i\\)’s quantity share and \\(V_i\\) is the (average) indirect utility that a consumer receives from purchasing product \\(i\\). We assume that \\(V_i\\) takes on the following form\n\\[\\begin{align*}\n  V_i=&\\delta_i + \\alpha p_i,&\\alpha<0.\n\\end{align*}\\]\nThe Logit demand system yields the following own- and cross-price elasticities:\n\\[\\begin{align*}\n  \\epsilon_{ii}=&\\alpha (1-s_i)p_i \\\\\n  \\epsilon_{ij}=&-\\alpha s_jp_j\n\\end{align*}\\]"
  },
  {
    "objectID": "2_merger_sim_intro.html#nested-logit",
    "href": "2_merger_sim_intro.html#nested-logit",
    "title": "What is Merger Simulation?",
    "section": "Nested Logit",
    "text": "Nested Logit\nBy construction, Logit demand assumes that diversion occurs according to quantity share. While convenient, one potential drawback of this assumption is that diversion according to share may not accurately represent consumer substitution patterns. One way to relax this assumption is to group the \\(n\\) products into \\(n > H \\ge 2\\) nests, with products in the same nest assumed to be closer substitutes than products in different nests.\nThe assumptions underlying nested Logit demand imply that the probability that a consumer purchases product \\(i\\) in nest \\(h\\in H\\) is given by\n\\[\\begin{align*}\n  s_i=& s_{i|h}s_h,&\\\\\n  s_{i|h}=&\\frac{\\exp(\\frac{V_i}{\\sigma_h})}{\\sum\\limits_{k \\in\n      h}\\exp(\\frac{V_k}{\\sigma_h})},& 1 \\ge \\sigma_h \\ge 0\\\\\n  s_{h}=& \\frac{\\exp(\\sigma_hI_h)}{\\sum\\limits_{l\\in H}\\exp(\\sigma_lI_l)},& I_h=\\log\\sum\\limits_{k \\in h}\\exp\\left(\\frac{V_k}{\\sigma_h}\\right).\n\\end{align*}\\]\nWe assume that \\(V_i\\) takes on the following form\n\\[\\begin{align*}\n  V_i=&\\delta_i + \\alpha p_i,& \\alpha\\le 0.\n\\end{align*}\\]\nThe Nested Logit demand system yields the following own- and cross-price elasticities:\n\\[\\begin{align*}\n  \\epsilon_{ii}=&\n    [1-s_i + (\\frac{1}{\\sigma_h}-1)(1-s_{i|h})]\\alpha p_i, \\\\\n  \\epsilon_{ij}=&\\begin{cases}\n    -[s_j + (\\frac{1}{\\sigma_h}-1)s_{j|h}]\\alpha p_j, &\n    \\text{if $i,j$ are both in nest $h$}.\\\\\n    -\\alpha s_jp_j, & \\text{if $i$ is not in nest $h$ but $j$ is}.\n  \\end{cases}\n\\end{align*}\\]\nNotice how these cross-price elasticities are identical to the non-nested Logit elasticities when products \\(i,j\\) are in different nests, but are larger when products \\(i,j\\) are in the same nests. This observation is consistent with the claim that products within a nest are closer substitutes than products outside of a nest."
  },
  {
    "objectID": "2_merger_sim_intro.html#useful-references",
    "href": "2_merger_sim_intro.html#useful-references",
    "title": "What is Merger Simulation?",
    "section": "Useful references",
    "text": "Useful references\nFor more information about the mathematical implementation and the theory, one can read\nBjörnerstedt and Verboven\nTo understand more about the Logit demand system (multinumial and nested logit) and the implementation in these pages its best to read berry 1994.\nTo get a wider perspective about possible implementations for antitrust practitioners, see the work of Taragin and Sandfort\nand finally, a very good source to understand the procedure of demand estimaion, a good place to start is Aviv nevo’s practitioner’s Guide"
  },
  {
    "objectID": "3_multinomial_logit.html",
    "href": "3_multinomial_logit.html",
    "title": "Multinomial Logit Example",
    "section": "",
    "text": "In this document I’ll demonstrate merger simulation with a synthetic example. On the supply side assuming a differentiated Bertrand model and on the demand side assuming a multinomial logit demand function.\nThe Bertrand system of equations has 4 components:\n(1) Market shares.\n(2) Prices.\n(3) Derivatives of the demand function with respect to the prices.\n(4) Marginal costs of the firms."
  },
  {
    "objectID": "3_multinomial_logit.html#fixed-point-iteration",
    "href": "3_multinomial_logit.html#fixed-point-iteration",
    "title": "Multinomial Logit Example",
    "section": "Fixed point iteration",
    "text": "Fixed point iteration\nThis change in the merged firm incentives has a ripple effect on the entire market. The firm will raise its prices, some of the customers will leave its products and either buy it at the competitors or not buy it at all. To compute this, we need an iterative process that will end in a new equilibrium.\nAssuming convexity and continuity of the equation system, we can find a solution and know it is unique.\naccording to the model , the average utility from product j is: \\[ \\delta_j = x \\beta_j - \\alpha p + \\xi _j \\]\nFor each product we calculate the average utility.\nA change in price will change the utility from the product. This in turn, will be reflected in the demand function, the demand decreases when the price goes up and vice versa.\n\n# delta = x * b - alpha * p\n(delta <- log(s / s_0)) \n\n[1] -0.223  0.000  0.182\n\nexp(delta) / (1 + sum(exp(delta)))\n\n[1] 0.20 0.25 0.30"
  },
  {
    "objectID": "3_multinomial_logit.html#manual-iteration",
    "href": "3_multinomial_logit.html#manual-iteration",
    "title": "Multinomial Logit Example",
    "section": "Manual iteration",
    "text": "Manual iteration\nWe’ll compute the first few iterations be hand to see how it converges.\nIn every iteration 4 stages will take place:\n\nSolve the firms first order conditions subject to market shares to find a new price vector.\nCalculate the change in the price vector compared to the pre-merger state.\nfeed the new prices to consumer utility functions to get the change in utilities.\nGet the new market shares out from the demand function.\n\n\n# the price update procedure: \n# 1. solve FOC 1\n(p1 <- as.vector(mc + (1/ - alpha) * (1 / (1 -  theta_post %*% s))))\n\n[1] 55.7 79.8 80.0\n\n# 2. delta price\n(d_p <- (p1 - p))\n\n[1] 5.68 4.85 0.00\n\n# 3. delta in utility\n(d_delta <- d_p * alpha)\n\n[1] -0.568 -0.485  0.000\n\n# 4. solve demand for new utility\n(s1 <- exp(delta + d_delta) / (1 + sum(exp(delta + d_delta))))\n\n[1] 0.139 0.188 0.367\n\n\nGet the results and compere to the pre merger status.\n\nresults <-rbind(c(s, p), c(s1, p1)) \nresults\n\n      [,1]  [,2]  [,3] [,4] [,5] [,6]\n[1,] 0.200 0.250 0.300 50.0 75.0   80\n[2,] 0.139 0.188 0.367 55.7 79.8   80\n\n\nRepeat the process several more times.\n\n# solve FOC 2:\np2 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s1)))\nd_p <- (p2 - p) \nd_delta <- d_p * alpha\n# solve demand 2:\ns2 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s2, p2))\n# solve FOC 3:\np3 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s2)))\nd_p <- (p3 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns3 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s3, p3))\n# solve FOC 4:\np4 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s3)))\nd_p <- (p4 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns4 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s4, p4))\n# solve FOC 5:\np5 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s4)))\nd_p <- (p5 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns5 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s5, p5))\n# solve FOC 6:\np6 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s5)))\nd_p <- (p6 - p) \nd_delta <- d_p * alpha\n# solve demand\ns6 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s6, p6))\n# solve FOC 7:\np7 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s6)))\nd_p <- (p7 - p) \nd_delta <- d_p * alpha\n# solve demand\ns7 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s7, p7))\n# solve FOC 8:\np8 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s7)))\nd_p <- (p8 - p) \nd_delta <- d_p * alpha\n# solve demand\ns8 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\n\nIn the results matrix, columns 1-3 are market shares and columns 4-6 are the prices.\nevery iteration the jumps in the values decreases.\n\nresults <- rbind(results, c(s8, p8))\ncolnames(results) <- c('s1', 's2', 's3', 'p1', 'p2', 'p3')\nresults\n\n         s1    s2    s3   p1   p2   p3\n [1,] 0.200 0.250 0.300 50.0 75.0 80.0\n [2,] 0.139 0.188 0.367 55.7 79.8 80.0\n [3,] 0.179 0.244 0.293 52.4 76.5 81.5\n [4,] 0.146 0.198 0.360 54.8 79.0 79.9\n [5,] 0.175 0.237 0.301 52.7 76.9 81.3\n [6,] 0.150 0.204 0.352 54.5 78.7 80.0\n [7,] 0.171 0.232 0.308 53.0 77.1 81.1\n [8,] 0.153 0.208 0.346 54.3 78.4 80.2\n [9,] 0.169 0.229 0.314 53.1 77.3 81.0\n\n\nWe can see where it’s going. the shares of products 1 and 2 are decreasing and product 3 is increasing. All prices are going up.\nLets calculate the new equilibrium."
  },
  {
    "objectID": "3_multinomial_logit.html#while-loop",
    "href": "3_multinomial_logit.html#while-loop",
    "title": "Multinomial Logit Example",
    "section": "While loop",
    "text": "While loop\nWe’ll limit the number of iterations with max_iter, and document the convergence process in a convergence_matrix .\n\nmax_iter <- 82 \ns_in <- s\ni <- 0\ns_delta_norm <- 1\nconvergence_matrix <- matrix(nrow = max_iter, ncol = length(s) * 2 + 2)\n\nwhile(s_delta_norm > 1e-6 & i < max_iter){\n  i <- i + 1\n  \n  # solve F.O.C\n  ( p_new <- as.vector(mc + (1/ - alpha ) * ( 1 / (1 -  theta_post %*% s_in) )))\n  \n  # change in utility\n  d_delta <- (p_new - p) * alpha\n  \n  # solve demand system\n  s_new <- exp(delta + d_delta) / (1 + sum(exp(delta + d_delta)))\n  \n  # norm of change in market shares\n  (s_delta_norm <- sqrt(sum((s_in - s_new) ^ 2)))\n  \n  # save resault for next iteration\n  s_in <- s_new\n  \n  convergence_matrix[i, ] <- c(s_new,  p_new, s_delta_norm, i)\n}\n\ncolnames(convergence_matrix) <- c(\"s1\", \"s2\", \"s3\", \"p1\", \"p2\", \"p3\", \"norm\", \"iteration\")\n\nconvergence_matrix\n\n         s1    s2    s3   p1   p2   p3        norm iteration\n [1,] 0.139 0.188 0.367 55.7 79.8 80.0 0.109827719         1\n [2,] 0.179 0.244 0.293 52.4 76.5 81.5 0.101188131         2\n [3,] 0.146 0.198 0.360 54.8 79.0 79.9 0.087867750         3\n [4,] 0.175 0.237 0.301 52.7 76.9 81.3 0.076132778         4\n [5,] 0.150 0.204 0.352 54.5 78.7 80.0 0.065389010         5\n [6,] 0.171 0.232 0.308 53.0 77.1 81.1 0.056301895         6\n [7,] 0.153 0.208 0.346 54.3 78.4 80.2 0.048324608         7\n [8,] 0.169 0.229 0.314 53.1 77.3 81.0 0.041574711         8\n [9,] 0.155 0.211 0.341 54.1 78.3 80.3 0.035693191         9\n[10,] 0.167 0.227 0.317 53.3 77.4 80.9 0.030698062        10\n[11,] 0.157 0.213 0.338 54.0 78.1 80.4 0.026361844        11\n[12,] 0.165 0.225 0.320 53.4 77.5 80.8 0.022667853        12\n[13,] 0.158 0.215 0.335 53.9 78.1 80.4 0.019469604        13\n[14,] 0.164 0.223 0.322 53.4 77.6 80.8 0.016738813        14\n[15,] 0.159 0.216 0.334 53.8 78.0 80.5 0.014379099        15\n[16,] 0.164 0.222 0.324 53.5 77.7 80.7 0.012360880        16\n[17,] 0.160 0.217 0.332 53.8 77.9 80.5 0.010619418        17\n[18,] 0.163 0.222 0.325 53.5 77.7 80.7 0.009128122        18\n[19,] 0.160 0.218 0.331 53.7 77.9 80.5 0.007842695        19\n[20,] 0.163 0.221 0.326 53.6 77.7 80.7 0.006740912        20\n[21,] 0.160 0.218 0.331 53.7 77.9 80.6 0.005791974        21\n[22,] 0.162 0.221 0.327 53.6 77.8 80.7 0.004978054        22\n[23,] 0.161 0.218 0.330 53.7 77.9 80.6 0.004277453        23\n[24,] 0.162 0.220 0.327 53.6 77.8 80.6 0.003676234        24\n[25,] 0.161 0.219 0.330 53.7 77.9 80.6 0.003158943        25\n[26,] 0.162 0.220 0.327 53.6 77.8 80.6 0.002714868        26\n[27,] 0.161 0.219 0.329 53.7 77.8 80.6 0.002332905        27\n[28,] 0.162 0.220 0.328 53.6 77.8 80.6 0.002004913        28\n[29,] 0.161 0.219 0.329 53.7 77.8 80.6 0.001722864        29\n[30,] 0.162 0.220 0.328 53.6 77.8 80.6 0.001480620        30\n[31,] 0.161 0.219 0.329 53.7 77.8 80.6 0.001272343        31\n[32,] 0.162 0.220 0.328 53.6 77.8 80.6 0.001093433        32\n[33,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000939630        33\n[34,] 0.162 0.220 0.328 53.6 77.8 80.6 0.000807498        34\n[35,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000693919        35\n[36,] 0.162 0.220 0.328 53.6 77.8 80.6 0.000596336        36\n[37,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000512461        37\n[38,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000440394        38\n[39,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000378453        39\n[40,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000325231        40\n[41,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000279488        41\n[42,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000240183        42\n[43,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000206402        43\n[44,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000177375        44\n[45,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000152428        45\n[46,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000130991        46\n[47,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000112568        47\n[48,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000096737        48\n[49,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000083132        49\n[50,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000071440        50\n[51,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000061393        51\n[52,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000052759        52\n[53,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000045339        53\n[54,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000038962        54\n[55,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000033483        55\n[56,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000028774        56\n[57,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000024727        57\n[58,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000021249        58\n[59,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000018261        59\n[60,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000015693        60\n[61,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000013486        61\n[62,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000011589        62\n[63,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000009959        63\n[64,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000008559        64\n[65,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000007355        65\n[66,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000006320        66\n[67,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000005432        67\n[68,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000004668        68\n[69,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000004011        69\n[70,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000003447        70\n[71,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002962        71\n[72,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002546        72\n[73,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002188        73\n[74,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001880        74\n[75,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001616        75\n[76,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001388        76\n[77,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001193        77\n[78,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001025        78\n[79,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000000881        79\n[80,]    NA    NA    NA   NA   NA   NA          NA        NA\n[81,]    NA    NA    NA   NA   NA   NA          NA        NA\n[82,]    NA    NA    NA   NA   NA   NA          NA        NA\n\n\nCompare the data before the merger with the prediction of the simulation about the merger effect on prices and market shares\n\n# attach preconditions with last row without NA's in the matrix\nfinal_resault <- \n  convergence_matrix[tail(which(rowSums(!is.na(convergence_matrix)) > 0), 1),]\n\n\nrbind(c(results[1,],norm = NA,iteration = 0),final_resault)\n\n                 s1    s2    s3   p1   p2   p3        norm iteration\n              0.200 0.250 0.300 50.0 75.0 80.0          NA         0\nfinal_resault 0.161 0.219 0.328 53.7 77.8 80.6 0.000000881        79\n\n\nThe market share of the outside option has increased:\n\nc(before = s_0, after = 1 - sum(final_resault[1:3]))\n\nbefore  after \n 0.250  0.291 \n\n\nPresent convergence process on a plot\n\npacman::p_load(tidyverse, patchwork)\n\np1 <- data.frame(convergence_matrix) %>%\n  select(iteration, p1, p2, p3) %>%\n  gather(k = \"k\", v = \"prices\", 2:4) %>%\n  ggplot(aes(x = iteration, y = prices, color = k)) + geom_line(size = 1.2)\n\np2 <- data.frame(convergence_matrix) %>%\n  select(iteration, s1, s2, s3) %>%\n  gather(k = \"k\", v = \"shares\", 2:4) %>%\n  ggplot(aes(x = iteration, y = shares, color = k)) + geom_line(size =1.2)\n\np1 / p2\n\n\n\n\nFirst order conditions:\n\ns_in + (theta_post*der) %*% (p_new - mc)\n\n         [,1]\n[1,] -0.01620\n[2,] -0.00271\n[3,]  0.01573\n\n\nThat’s it.\nIn the next example we’ll show a bit more complicated example of One level Nested logit. Also, the code will be warped up in functions for production."
  },
  {
    "objectID": "4_nested.html",
    "href": "4_nested.html",
    "title": "Nested Logit",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\noptions(scipen = 999, digits = 3)"
  },
  {
    "objectID": "4_nested.html#data-preparation",
    "href": "4_nested.html#data-preparation",
    "title": "Nested Logit",
    "section": "2.1. Data preparation",
    "text": "2.1. Data preparation\nPrepare data set - f1_init_calibration creates a new data frame in the global environment with necessary variables for the simulation.\nThe new data frame’s name is calibration data.\nWhen calling the function all arguments must be specified:\n(1) data frame that includes the columns listed next. (2) p: product prices. (3) q: quantities sold for each product. (4) firm: the firm that owns the product. (5) nest: the group of products the product belongs to according to the model specified the the researcher. (6) m_size: The market size assumed by the researcher. (The choice of market size actually determines the size of the outside option. This choice has a decisive effect on the results of the simulation, but this subject is beyond the scope of this document). (7) buyer: The acquirer firm. (8) seller: the purchased firm.\n\nf1_init_calibration <- function(data, p, q, firm, nest, m_size, buyer, seller){\n  \n  data <- data %>%\n    rename(p = {{p}}, q = {{q}},firm = {{firm}}, nest = {{nest}}, m_size = {{m_size}})\n  \n  nest_mat <<- outer(data$nest, data$nest, FUN = \"==\") * 1 # nest matrix\n  \n  calibration_data <<-\n    data %>%\n    select(p, q, firm, nest, m_size) %>%\n    mutate(\n      p = as.numeric(p),\n      q = as.numeric(q),\n      s = q / m_size,                   # product market share \n      s_0 = 1 - sum(s),                 # outside option market share\n      s_g = as.numeric(nest_mat %*% s), # share of each nest\n      s_jg = s / s_g,                   # share of product j in nest g\n      firm_post = if_else(firm == {{seller}}, {{buyer}}, firm),\n      n = n()) %>% na.omit()\n}"
  },
  {
    "objectID": "4_nested.html#derivative-matrix",
    "href": "4_nested.html#derivative-matrix",
    "title": "Nested Logit",
    "section": "2.2. Derivative matrix",
    "text": "2.2. Derivative matrix\nf2_derivatives builds and returns the derivative matrix from the shares, alpha and sigma.\nIn one level nested-logit there are 3 types of derivatives:\n(1) Own derivative (which should be all negative -> demand goes down as price goes up)\n(2) cross derivative for products in the same nest.\n(3) cross derivative for products in a different nest. (These are the same as the cross derivatives in the Multinomial Logit model)\n\nf2_derivatives <- function(shares, nest_shares, alpha, sigma){\n  \n  # preparations\n  s     <- shares\n  s_jg  <- nest_shares\n  n     <- length(s)\n  alpha <- - abs(alpha) # to avoid confusion. fix the sign of alpha\n  \n  # derivatives\n  cross_different  <- - alpha * s %o% s * (1 - nest_mat)\n  \n  cross_same <- \n    - alpha * (sigma / (1 - sigma) * s_jg + s) %o% s * nest_mat *\n    (1 - diag(x = 1,nrow = n))\n  \n  own <- alpha * s * (1 / (1-sigma) - sigma / (1-sigma) * s_jg - s)\n  \n  # warp it in the matrix\n  derivatives <- cross_different + cross_same + diag(own)\n  derivatives\n}"
  },
  {
    "objectID": "4_nested.html#pre-merger-conditions",
    "href": "4_nested.html#pre-merger-conditions",
    "title": "Nested Logit",
    "section": "2.3. Pre-merger conditions",
    "text": "2.3. Pre-merger conditions\nf3_pre_merger_conditions gets as inputs the calibration_data we created in the global environment, a derivatives matrix and the assumed \\(sigma\\), the parameter for correlation between products in the same nest.\nf3_pre_merger_conditions solves the first order condition of the equation system, and calculates several supplemented variables:\n(1) Profit margins and (2) Lerner index to check profitability, the (3) Delta which is the mean utility the consumer has with the prevailing prices and a (4) FOC condition to see if the iteration process that will come next converges to zero.\nThe new variables are added the the calibration_data in the global environment.\n\nf3_pre_merger_conditions <- function(c_data, derivatives, sigma){\n  \n  theta <- outer(c_data$firm, c_data$firm, FUN = \"==\") * 1 # ownership matrix\n  s <- c_data$s\n  p <- c_data$p\n\n  # system solution for marginal costs  \n  c_data$mc <- solve(derivatives * theta) %*% s + p \n  \n  c_data <- c_data %>%\n    mutate(margin = p - mc,\n           lerner_index = (p - mc) / p,\n           FOC = as.vector(s + (theta * derivatives) %*% (p - mc)),\n           delta =  log(s / s_0)  - sigma * log(s_jg)\n           )\n  \n  calibration_data <<- c_data\n}"
  },
  {
    "objectID": "4_nested.html#simulate-the-merger",
    "href": "4_nested.html#simulate-the-merger",
    "title": "Nested Logit",
    "section": "2.4. Simulate the merger",
    "text": "2.4. Simulate the merger\n\n2.4a. Nested logit Demand function\nIn the process of looking for a new equilibrium after the merger, we need to use the consumers demand function. Every time the prices change, the mean utility of the consumer changes, hence the quantity demand will change. f5_demand is the demand function. It calculates the market shares that will prevail given a vector of prices. Its arguments are delta- the mean utility from consumers have for each product, and \\(sigma\\) - the in-nest correlation parameter according to the nested logit model.\nThe user need not operate this function. It will be called from within f4_fixed_point that will calculate the new equilibrium.\n\nf5_demand <- function(delta, sigma){\n  \n  # demand function\n  exponent    <- exp(delta /(1-sigma))\n  D_g         <- unique(nest_mat, MARGIN = 1) %*%  exponent\n  denominator <- D_g ^ sigma * (1 + sum(D_g^(1-sigma)))\n  s_t         <- as.vector(exponent * unique(nest_mat, MARGIN = 2) %*% (1/ as.vector(denominator)))\n  \n  # nest shares\n  s_g  <- as.numeric(nest_mat %*% s_t)  # nest market share\n  s_jg <- s_t / s_g                     # share within the nest\n  \n  data.frame(s_t = s_t, s_jg_t = s_jg, delta_t = delta, n = length(s_t))\n}\n\n\n\n2.4b. Fixed point iteration\nf4_fixed_point calculates the new equilibrium with a fixed point iteration algorithm.\nIt uses the f5_demand f2_derivatives and solves the FOC iteratively until solution is reached or until the maximum number of iterations is reached.\nThe user has control over the maximum number of iterations and the tolarance desired for convergence.\nwe shall expand about the argument convergence_factor later.\n\nf4_fixed_point <- \n  function(data, alpha, sigma, max_iter = 100,\n           convergence_factor = 1, tolerance = 1e-3){\n    \n    theta_post <- outer(data$firm_post, data$firm_post, FUN = \"==\") * 1\n    s_in       <- data$s\n    p          <- data$p\n    mc         <- data$mc\n    delta      <- data$delta\n    der_new    <- f2_derivatives(calibration_data$s, calibration_data$s_jg, alpha, sigma)\n    log         <- matrix(nrow = max_iter, ncol = length(s_in) + 2)\n    i          <- 0\n    s_d_norm   <- 1\n    \n    while(i < max_iter & s_d_norm > tolerance){\n      \n      i         <- i + 1\n      p_new     <- mc - (solve(der_new * theta_post)) %*% s_in  # new price\n      delta_new <- delta - abs(alpha) * as.vector((p_new - p)/ convergence_factor)\n      s_new     <- f5_demand(delta_new, sigma)\n      der_new   <- f2_derivatives(s_new$s_t, s_new$s_jg_t, alpha, sigma)\n      s_d_norm  <- sqrt(sum((s_in - s_new[[1]]) ^ 2)) # measure convergence\n      s_in      <- s_new[[1]]    # new price vector to feed in\n      log[i,]    <- c(p_new, norm = s_d_norm,iteration = i) # results\n      \n    }\n    \n    log       <- data.frame(log) %>% drop_na()\n    names(log)<- c(paste(\"p\", 1:length(s_in), sep = \"_\"), \"norm\", \"iter\")\n    simulation_log <<- log\n    \n    tail     <- tail(log,1)\n    \n    data$new_prices <- tail %>% select(starts_with(\"p_\")) %>% t() %>% as.vector()\n    data$iterations <- tail %>% select(iter) %>% pull()\n    data$norm       <- tail %>% select(norm) %>% pull()\n    data$new_shares <- s_new[[1]]\n    \n    calibration_results <<- data %>%\n      select(firm, nest, p, new_prices, s, new_shares,\n             iterations, norm, q, m_size)\n    \n    \n  }"
  },
  {
    "objectID": "4_nested.html#build-toy-data",
    "href": "4_nested.html#build-toy-data",
    "title": "Nested Logit",
    "section": "3.1 Build toy data",
    "text": "3.1 Build toy data\nIts about time to see it all in action.\nwe’ll create a toy data of 6 products owned by 3 firms, divided into 2 nests.\nmarket size will be 100.\n\ndf <- data.frame(\n  company  = c(\"a\",     \"a\",     \"b\",    \"b\", \"c\", \"c\"),\n  nests    = c(\"pre\",   \"pre\",   \"pre\",   2,   2,   2),\n  quantity = c( 20,      5,       10,     5,   10,  25),\n  price    = c( 60,      40,      50,     45,  30,  30),\n  m_size   = 100,\n  not_needed_variable = \"junk\")\ndf\n\n  company nests quantity price m_size not_needed_variable\n1       a   pre       20    60    100                junk\n2       a   pre        5    40    100                junk\n3       b   pre       10    50    100                junk\n4       b     2        5    45    100                junk\n5       c     2       10    30    100                junk\n6       c     2       25    30    100                junk\n\n\nWe can prepare the data for simulation using f1_init_calibration:\nNote that all argument are needed for the function to know which variable is which:\nprices, quantities, firm, nest, market size, buyer and seller.\n\nf1_init_calibration(df, p = price, q = quantity, firm = company, nest = nests,\n                    m_size = m_size, buyer = 'a', seller = 'b')\n\nA new df namedcalibration_data was created in the global environment:\n\ncalibration_data\n\n   p  q firm nest m_size    s  s_0  s_g  s_jg firm_post n\n1 60 20    a  pre    100 0.20 0.25 0.35 0.571         a 6\n2 40  5    a  pre    100 0.05 0.25 0.35 0.143         a 6\n3 50 10    b  pre    100 0.10 0.25 0.35 0.286         a 6\n4 45  5    b    2    100 0.05 0.25 0.40 0.125         a 6\n5 30 10    c    2    100 0.10 0.25 0.40 0.250         c 6\n6 30 25    c    2    100 0.25 0.25 0.40 0.625         c 6"
  },
  {
    "objectID": "4_nested.html#assume-regression-results",
    "href": "4_nested.html#assume-regression-results",
    "title": "Nested Logit",
    "section": "3.2. Assume regression results",
    "text": "3.2. Assume regression results\nIf one does a Nested Logit demand estimation successfully , one has the right \\(sigma\\) and \\(alpha\\) to put into the simulation. here, we assume those parameters to be:\n\nsigma0.5 <- 0.5\nalpha0.1 <- 0.1\n\nThis does not have to be an arbitrary assumption, rather it can be based on the knowledge we have about the market."
  },
  {
    "objectID": "4_nested.html#checking-derivatives",
    "href": "4_nested.html#checking-derivatives",
    "title": "Nested Logit",
    "section": "3.3. Checking Derivatives",
    "text": "3.3. Checking Derivatives\nLets see how the derivative matrix is like for this toy data.\nIn actual simulation this function is being called from f4_fixed_point, so there is no need to call it.\n\nder <- f2_derivatives(calibration_data$s, calibration_data$s_jg, alpha0.1, sigma0.5)\nder\n\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n[1,] -0.02457  0.00386  0.00771  0.00100  0.00200  0.00500\n[2,]  0.00386 -0.00904  0.00193  0.00025  0.00050  0.00125\n[3,]  0.00771  0.00193 -0.01614  0.00050  0.00100  0.00250\n[4,]  0.00100  0.00025  0.00050 -0.00913  0.00175  0.00437\n[5,]  0.00200  0.00050  0.00100  0.00175 -0.01650  0.00875\n[6,]  0.00500  0.00125  0.00250  0.00438  0.00875 -0.02813\n\n\nGlimpsing on it, it look like the own derivative in the diagonal is negative and the rest is positive."
  },
  {
    "objectID": "4_nested.html#pre-merger-conditions-1",
    "href": "4_nested.html#pre-merger-conditions-1",
    "title": "Nested Logit",
    "section": "3.4. Pre-merger conditions",
    "text": "3.4. Pre-merger conditions\nNow we calculate the marginal costs of the firms. all new variables are added to the calibration_data .\n\nf3_pre_merger_conditions(calibration_data, der, sigma0.5)\ncalibration_data\n\n   p  q firm nest m_size    s  s_0  s_g  s_jg firm_post n   mc margin\n1 60 20    a  pre    100 0.20 0.25 0.35 0.571         a 6 50.3   9.66\n2 40  5    a  pre    100 0.05 0.25 0.35 0.143         a 6 30.3   9.66\n3 50 10    b  pre    100 0.10 0.25 0.35 0.286         a 6 43.6   6.38\n4 45  5    b    2    100 0.05 0.25 0.40 0.125         a 6 39.2   5.83\n5 30 10    c    2    100 0.10 0.25 0.40 0.250         c 6 17.1  12.90\n6 30 25    c    2    100 0.25 0.25 0.40 0.625         c 6 17.1  12.90\n  lerner_index                    FOC   delta\n1        0.161  0.0000000000000000000  0.0567\n2        0.241  0.0000000000000000000 -0.6365\n3        0.128 -0.0000000000000000278 -0.2899\n4        0.130  0.0000000000000000208 -0.5697\n5        0.430  0.0000000000000000000 -0.2231\n6        0.430  0.0000000000000000000  0.2350"
  },
  {
    "objectID": "4_nested.html#check-the-demand-system",
    "href": "4_nested.html#check-the-demand-system",
    "title": "Nested Logit",
    "section": "3.5. Check the demand system",
    "text": "3.5. Check the demand system\nFor the sake of presentation, lets see that the demand function works correctly.\nFeeding the delta’s consumers had, we should get the same market shares we assumed.\nEvery time the price will change, the delta will change and so the shares.\n\nf5_demand(calibration_data$delta, sigma0.5)\n\n   s_t s_jg_t delta_t n\n1 0.20  0.571  0.0567 6\n2 0.05  0.143 -0.6365 6\n3 0.10  0.286 -0.2899 6\n4 0.05  0.125 -0.5697 6\n5 0.10  0.250 -0.2231 6\n6 0.25  0.625  0.2350 6\n\n\nWe got the same market shares we chose when we first built our toy data, so its OK."
  },
  {
    "objectID": "4_nested.html#simulate-the-merger-1",
    "href": "4_nested.html#simulate-the-merger-1",
    "title": "Nested Logit",
    "section": "3.6. Simulate the merger",
    "text": "3.6. Simulate the merger\nAt last, we can simulate the merger.\nUsing the fixed point iteration, we get a new df named calibration_results with the results of the simulation.\n\nf4_fixed_point(calibration_data, alpha0.1, sigma0.5, convergence_factor = 1.2)\ncalibration_results\n\n  firm nest  p new_prices    s new_shares iterations     norm  q m_size\n1    a  pre 60       64.5 0.20     0.1736         89 0.000972 20    100\n2    a  pre 40       44.5 0.05     0.0434         89 0.000972  5    100\n3    b  pre 50       57.8 0.10     0.0502         89 0.000972 10    100\n4    b    2 45       46.7 0.05     0.0486         89 0.000972  5    100\n5    c    2 30       30.9 0.10     0.1109         89 0.000972 10    100\n6    c    2 30       30.9 0.25     0.2772         89 0.000972 25    100\n\n\ncalibration_results gives the new prices and new shares, reports how many iterations were needed to converge to the tolerance of 1/1000 and reports the norm of the change in prices in the last iteration.\nWe can watch the entire process in the simulation_log object created in the global environment.\n\nrbind(head(simulation_log),tail(simulation_log))\n\n    p_1  p_2  p_3  p_4  p_5  p_6     norm iter\n1  66.4 46.4 59.7 47.7 30.0 30.0 0.098581    1\n2  63.7 43.7 57.0 46.1 32.4 32.4 0.082170    2\n3  65.2 45.2 58.5 47.2 29.5 29.5 0.077062    3\n4  64.0 44.0 57.2 46.3 32.4 32.4 0.073149    4\n5  65.1 45.1 58.3 47.1 29.6 29.6 0.069658    5\n6  64.0 44.0 57.3 46.3 32.2 32.2 0.066111    6\n84 64.5 44.5 57.8 46.7 30.9 30.9 0.001254   84\n85 64.5 44.5 57.8 46.7 30.9 30.9 0.001191   85\n86 64.5 44.5 57.8 46.7 30.9 30.9 0.001132   86\n87 64.5 44.5 57.8 46.7 30.9 30.9 0.001076   87\n88 64.5 44.5 57.8 46.7 30.9 30.9 0.001023   88\n89 64.5 44.5 57.8 46.7 30.9 30.9 0.000972   89\n\nsimulation_log %>% select( - norm) %>%\n  pivot_longer(1:6) %>%\n  ggplot(aes(x = iter, y = value, color = name)) + geom_line()\n\n\n\n\nNote that when we called f4_fixed_point we used a convergenc_factor of 1.2.\nAs it happens, this data consists of only 6 observations and the fixed point algorithm doesn’t converge naturally with a convergence_factor of 1.\nAt best practice, one should use a convergence_factor of 1, and only if there’s a problem to choose a convergence_factor > 1.\nLets see what happens if we chose convergence_factor = 1.\n\nf4_fixed_point(calibration_data, alpha0.1, sigma0.5, convergence_factor = 1)\n\nPrices and shares are constantly jumping between two points.\n\nsimulation_log %>% select( - norm) %>%\n  pivot_longer(1:6) %>%\n  ggplot(aes(x = iter, y = value, color = name)) + geom_line()\n\n\n\n\ncalibration_resaults also tells us that the norm is far far away from zero:\n\ncalibration_results\n\n  firm nest  p new_prices    s new_shares iterations  norm  q m_size\n1    a  pre 60       60.9 0.20    0.28013        100 0.637 20    100\n2    a  pre 40       40.9 0.05    0.07003        100 0.637  5    100\n3    b  pre 50       54.2 0.10    0.07268        100 0.637 10    100\n4    b    2 45       44.5 0.05    0.20641        100 0.637  5    100\n5    c    2 30       52.8 0.10    0.00387        100 0.637 10    100\n6    c    2 30       52.8 0.25    0.00967        100 0.637 25    100"
  },
  {
    "objectID": "equations.html",
    "href": "equations.html",
    "title": "formulas",
    "section": "",
    "text": "Logit demand function\nTheir are \\(j\\) products, indexed by \\(j = 1, ...,J\\) . The demand for product \\(j\\) is \\(q_j(\\mathbf{p})\\). \\(\\mathbf{p}\\) is a \\(J X 1\\) price vector and the costs of producing the the product is \\(c_j\\). Each firm \\(f\\) owns a subset of products \\(F_f\\) and chooses its own prices to maximize\n\\[\n\\Pi_f(\\mathbf{p}) = \\sum_{j \\in F_f}(p_j -c_j)q_j (\\mathbf{p}) + \\sum_{j \\notin F_f}(p_j -c_j )q_j(\\mathbf{p})     \n\\]\nA Bertrand- Nash equilibrium equilibrium is defined by the system of first order conditions:\n\\[\nq_j(\\mathbf{p}) + \\sum_{k \\in F_f}(p_k - c_k)\n\\frac{\\partial q_k(\\mathbf{p})}{\\partial p_j} +\nq_j(\\mathbf{p}) + \\sum_{k \\notin F_f}(p_k - c_k)\n\\frac{\\partial q_k(\\mathbf{p})}{\\partial p_j},\nj = 1, ...,J\n\\]\ndefine \\(\\theta\\) to by a \\(JXJ\\) product ownership matrix with \\(\\theta(j,k) =1\\) if products \\(j\\) and \\(k\\) are owned by the same firm and \\(0\\) otherwise. let \\(q(\\mathbf{p})\\) be the demand vector,\n\\(\\Delta\\mathbf{p} ==\\frac{\\partial q(\\mathbf{p})}{\\partial p^d}\\)\nbe the \\(jxj\\) Jacobian of first derivatives and c be the jX1 marginal cost vectr we can write is vector notation as:\nFirm \\(k \\in K\\) chooses the prices \\(\\{p_i\\}_{i=1}^{n_k}\\) of its products so as to maximize profits. Mathematically, firm \\(k\\) solves:\n\\[\\begin{align*}\n\\max_{\\{p_i\\}_{i=1}^{n_k}} &\\sum_{i=1}^{n_k}(\\omega_{ik})(p_i - c_i)q_i,\n\\end{align*}\\]\nwhere \\(\\omega_{ik}\\) is the share of product \\(i\\)’s profits earned by firm \\(k\\), so that \\(\\sum\\limits_{k\\in K} \\omega_{ik}\\le 1\\). \\(q_i\\), the quantity sold of product \\(i\\), is assumed to be a twice differentiable function of all product prices.\nDifferentiating profits with respect to each \\(p_i\\) yields the following first order conditions (FOCs):\n\\[\\begin{align*}\n  \\partial p_i&\\equiv \\omega_{ik}q_i +\\sum_{j=1}^{n}\\omega_{jk}( p_j - c_j)\\frac{\\partial q_j}{\\partial\n    p_i}=0& \\mbox{ for all $i\\in n_k$}\n\\end{align*}\\]\nwhich may be rewritten as\n\\[\\begin{align*}\n  \\partial p_i&\\equiv \\omega_{ik}r_i + \\sum_{j=1}^{n} \\omega_{jk}r_jm_j\\epsilon_{ji}=0& \\mbox{ for all $i\\in n_k$},\n\\end{align*}\\]\nwhere \\(r_i\\equiv\\frac{p_iq_i}{\\sum\\limits_{j=1}^np_jq_j}\\) is product \\(i\\)’s revenue share, \\(m_i\\equiv\\frac{p_i-c_i}{p_i}\\) is product \\(i\\)’s gross margin, and \\(\\epsilon_{ij}\\equiv\\frac{\\partial q_i}{\\partial p_j}\\frac{p_j}{q_i}\\) is the elasticity of product \\(i\\) with respect to the price of product \\(j\\).\nThe FOCs for all products may be stacked and then represented using the following matrix notation: \\[\\begin{align*}\n  (r\\circ diag(\\Omega)) + (E\\circ\\Omega)'(r \\circ m)=0\n  (\\#eq:FOC1)\n\\end{align*}\\]\nrearranging yields\n\\[\\begin{align*}\n  m_{Bertrand}= -\\{(E\\circ\\Omega)'^{-1}(r\\circ diag(\\Omega))\\} \\circ \\frac{1}{r}\n  (\\#eq:FOC2)\n\\end{align*}\\]\nwhere \\(r\\) and \\(m\\) are \\(n\\)-length vectors of revenue shares and margins, \\(E = \\left(\\begin{smallmatrix} \\epsilon_{11}&\\ldots&\\epsilon_{1n}\\\\\\vdots &\\ddots&\\vdots\\\\\\epsilon_{n1}&\\ldots&\\epsilon_{nn} \\end{smallmatrix}\\right)\\) is a \\(n \\times n\\) matrix of own- and cross-price elasticities, and \\(\\Omega=\\left(\\begin{smallmatrix} \\omega_{11}&\\ldots&\\omega_{1n}\\\\\\vdots &\\ddots&\\vdots\\\\\\omega_{n1}&\\ldots&\\omega_{nn} \\end{smallmatrix}\\right)\\) is an \\(n \\times n\\) matrix whose \\(i,j\\)th element equals the share of product \\(j\\)’s profits owned by the firm setting product \\(i\\)’s price.1 In many cases, product \\(i\\) and \\(j\\) are wholly owned by a single firm, in which cases the \\(i,j\\)th element of \\(\\Omega\\) equals 1 if \\(i\\) and \\(j\\) are owned by the same firm and 0 otherwise. Under partial ownership, the columns of the matrix formed from the unique rows of \\(\\Omega\\) must sum to 1. ‘\\(diag\\)’ returns the diagonal of a square matrix and ‘\\(\\circ\\)’ is the Hadamard (entry-wise) product operator.\nThe solution to system @ref(eq:FOC1) yields equilibrium prices conditional on the ownership structure \\(\\Omega\\). A (partial) merger is modeled as the solution to system @ref(eq:FOC1) where \\(\\Omega\\) is changed to reflect the change in ownership.\n\n\nlogit\nLogit demand is based on a discrete choice model that assumes that each consumer is willing to purchase at most a single unit of one product from the \\(n\\) products available in the market. The assumptions underlying Logit demand imply that the probability that a consumer purchases product \\(i \\in n\\) is given by\n\\[\\begin{align*}\n  s_i=& \\frac{\\exp(V_i)}{\\sum\\limits_{k \\in n}\\exp(V_k)},&\n\\end{align*}\\]\nwhere \\(s_i\\) is product \\(i\\)’s quantity share and \\(V_i\\) is the (average) indirect utility that a consumer receives from purchasing product \\(i\\). We assume that \\(V_i\\) takes on the following form\n\\[\\begin{align*}\n  V_i=&\\delta_i + \\alpha p_i,&\\alpha<0.\n\\end{align*}\\]\nThe Logit demand system yields the following own- and cross-price elasticities:\n\\[\\begin{align*}\n  \\epsilon_{ii}=&\\alpha (1-s_i)p_i \\\\\n  \\epsilon_{ij}=&-\\alpha s_jp_j\n\\end{align*}\\]\n\n\nnested\nBy construction, Logit demand assumes that diversion occurs according to quantity share. While convenient, one potential drawback of this assumption is that diversion according to share may not accurately represent consumer substitution patterns. One way to relax this assumption is to group the \\(n\\) products into \\(n > H \\ge 2\\) nests, with products in the same nest assumed to be closer substitutes than products in different nests.2 logit.nests‘s ’nests’ argument may be used to specify a length-\\(n\\) vector identifying which nest each product belongs to.\nThe assumptions underlying nested Logit demand imply that the probability that a consumer purchases product \\(i\\) in nest \\(h\\in H\\) is given by\n\\[\\begin{align*}\n  s_i=& s_{i|h}s_h,&\\\\\n  s_{i|h}=&\\frac{\\exp(\\frac{V_i}{\\sigma_h})}{\\sum\\limits_{k \\in\n      h}\\exp(\\frac{V_k}{\\sigma_h})},& 1 \\ge \\sigma_h \\ge 0\\\\\n  s_{h}=& \\frac{\\exp(\\sigma_hI_h)}{\\sum\\limits_{l\\in H}\\exp(\\sigma_lI_l)},& I_h=\\log\\sum\\limits_{k \\in h}\\exp\\left(\\frac{V_k}{\\sigma_h}\\right).\n\\end{align*}\\]\nWe assume that \\(V_i\\) takes on the following form\n\\[\\begin{align*}\n  V_i=&\\delta_i + \\alpha p_i,& \\alpha\\le 0.\n\\end{align*}\\]\nThe Nested Logit demand system yields the following own- and cross-price elasticities:\n\\[\\begin{align*}\n  \\epsilon_{ii}=&\n    [1-s_i + (\\frac{1}{\\sigma_h}-1)(1-s_{i|h})]\\alpha p_i, \\\\\n  \\epsilon_{ij}=&\\begin{cases}\n    -[s_j + (\\frac{1}{\\sigma_h}-1)s_{j|h}]\\alpha p_j, &\n    \\text{if $i,j$ are both in nest $h$}.\\\\\n    -\\alpha s_jp_j, & \\text{if $i$ is not in nest $h$ but $j$ is}.\n  \\end{cases}\n\\end{align*}\\]\nNotice how these cross-price elasticities are identical to the non-nested Logit elasticities when products \\(i,j\\) are in different nests, but are larger when products \\(i,j\\) are in the same nests. This observation is consistent with the claim that products within a nest are closer substitutes than products outside of a nest.\nIn contrast to nested LA-AIDS, which must calibrate \\(\\frac{H(H-1)}{2}\\) nesting parameters, only \\(H\\) nesting parameters must be calibrated. By default, calcSlopes constrains all the nesting parameters to be equal to one another, \\(\\sigma_h=\\sigma\\) for all \\(h\\in H\\). This reduces the number of parameters that need to be estimated to \\(n+2\\) (\\(n\\) \\(\\delta\\)s, \\(\\alpha,\\sigma\\)) which means users must furnish enough margin information to complete at least two FOCs. Setting logit.nests‘s ’constraint’ argument to FALSE causes the calcSlopes method to relax the constraint and calibrate a separate nesting parameter for each nest. Relaxing the constraint increases the number of parameters that must be estimated to \\(n+H+1\\), which means that users must furnish margin information sufficient to complete at least \\(H+1\\) FOCs. Moreover, users must supply at least one margin per nest for each non-singleton nest. In other words, if nest \\(h\\in H\\) contains \\(n_h>1\\) products, then at least one product margin from nest \\(h\\) must be supplied.\n\n\n\n\n\nFootnotes\n\n\nThe Bertrand model assumes that while any firm can receive a portion of another firm’s profits (e.g. through owning a share of that firms’ assets), only one firm can set a product’s price.↩︎\nNo function in antitrust currently permits a hierarchy of nests. Singleton nests (nests containing only a single product) are technically permitted, but their nesting parameter is not identified and is therefore normalized to 1.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eviatar Guttman",
    "section": "",
    "text": "disclaimer\n\n\n\nThe views expressed herein are entirely mine and should not be purported to reflect those of the Israel competition Authority.\n\n\n\nIn a research project called Information problems in the mortgage market in Israel: price dispersion and search (sadly, the paper was published solely in English though there is an extended abstract in here) we predicted the entire counter-factual price distribution of mortgage loans had the customers would have asked for more bank bids in a friction-less environment. The code example shown here replicates the core idea of performing 297 quantile regressions and using its predictions to build a new hypothetical price distribution through simulation.\nMerger simulation is a framework that uses economic models grounded in the theory of industrial organization to predict the effect of mergers on prices in markets. Merger simulation is at the frontier of the empirical economics used by competition authorities. The examples shown here are predominant techniques in merger simulation implementation. We starts with a little bit of background, continue with a simplified example for clarity and finishes with a more advanced example accompanied with production-level code.\n\nMy intent is to expand with more examples in the future as time will allow.\nYou can contact me at eviatargu@competition.gov.il"
  },
  {
    "objectID": "merger_sim_intro.html",
    "href": "merger_sim_intro.html",
    "title": "What is Merger Simulation?",
    "section": "",
    "text": "The next pages presents examples of horizontal merger simulation implemented in R code.\nMerger simulation is a quantitative tool to create prediction about the probable outcome effect of a merger on competition. The process can be divided to three parts:\n(1) Selection of a structural model of supply and demand.\n(2) Calibration model’s parameters.\n(3) Computation of new equilibrium that will prevail post merger.\nNext, I’ll briefly describe the selected three parts chosen for the forthcoming examples. Afterwards, a brief view of the equation system that emerge from the model, and finally, a few references for those who’d like to deepen their knowledge in the subject.\n(1) The structural model\nOn the supply side we’ll use a Bertrand model with differentiated products. On the demand side versions of logit demand systems will be used. This specification is common for competition agencies to work with.\n(2) model calibration\nTwo techniques are common, the harder one is implementing demand estimation of the demand function parameters. It is a demanding process both in terms of the required data and in terms of the required scope of work. Another approach is to calibrate the model using carefully selected parameters - this is what we’ll do here.\n(3) Equilibrium computation\nIn the case of a non-linear demand system such as we have here, no analytic solution exists. Some kind of numeric method is required to solve the equation system - here I’ll use fixed point iteration."
  },
  {
    "objectID": "merger_sim_intro.html#equation-system-of-the-structural-model",
    "href": "merger_sim_intro.html#equation-system-of-the-structural-model",
    "title": "What is Merger Simulation?",
    "section": "Equation system of the structural model",
    "text": "Equation system of the structural model\nNext, we explain a little bit about the chosen structural model. first, the equations of the Bertrand model followed by the simple logit demand system.\n\nBertrand model with differentiated products\nA very good explanation of how the betrand model works is written by Jonas Björnerstedt (Swedish competition authority) and Frank Verboven (University of Leuven) in their merger simulation implementation in stata. The next few images are taken from section 2 of their paper:\n\n\n\n\n\nThe multinomial logit model\nLogit demand is based on a discrete choice model that assumes that each consumer is willing to purchase at most a single unit of one product from the \\(n\\) products available in the market. The assumptions underlying Logit demand imply that the probability that a consumer purchases product \\(j \\in n\\) is given by\n\n\n\n\n\nwhere \\(s_j\\) is product \\(j\\)’s quantity share and \\(V_j\\) is the (average) indirect utility that a consumer receives from purchasing product \\(j\\). We assume that \\(V_j\\) takes on the following form:\n\n\n\n\n\nThe Logit demand system yields the following own- and cross-price elasticities:"
  },
  {
    "objectID": "merger_sim_intro.html#nested-logit-model",
    "href": "merger_sim_intro.html#nested-logit-model",
    "title": "What is Merger Simulation?",
    "section": "Nested logit model",
    "text": "Nested logit model\nthe nested logit is a generalization of the multinomial model…. still need to sum it up."
  },
  {
    "objectID": "merger_sim_intro.html#usefull-references",
    "href": "merger_sim_intro.html#usefull-references",
    "title": "What is Merger Simulation?",
    "section": "Usefull references",
    "text": "Usefull references\nFor more information about the mathematical implementation and the theory, one can read\nBjörnerstedt and Verboven\nTo understand more about the Logit demand system (multinumial and nested logit) and the implementation is this repository its best to read berry 1994.\nTo get a wider perspective about possible implementations for antitrust practitioners, see the work of Taragin and Sandfort\nand finally, a very good source to understand the procedure of demant estimaion, a good place to start is Aviv nevo’s practitioner’s Guide"
  },
  {
    "objectID": "mortgage.html",
    "href": "mortgage.html",
    "title": "What we did in the mortgage research",
    "section": "",
    "text": "The algorithm\nThis example will replicate the core idea of the simulation.\nWe will do 99 quantile regressions for each of the 3 banks data-sets. then, relying of the fact the data-sets are unified, for each observation we will predict the price over 297 regression models, 99 for each bank. This will allow a reconstruction of a predicted price distribution for each loan in each of the 3 banks.\nOnce we have the price distributions for each loan two types of comparisons are possible:\n\nWe assume that a consumer’s quantile is predetermined. that is, the researcher has some unobservables which don’t exist in the data but the customer and the banks know the values of these unobservables. In this case, if a customer belongs to the 42nd quantlie, than we should compare the 42nd quantile between the 3 banks. To measure the expected savings from bidding between banks sums up to taking the minimum from each quantile of the 3 price distributions.\nMore interesting to show in this document is the case where the customer’s quantiles are not determined but random. conditional on the customer and transaction’s characteristics she could be in quantile 42 in bank A, quantile 14 in bank B and quantile 75 in bank C. To simulate this process we need to draw a triplet of quantiles, one for each bank. If we draw enough triplets we can re-construct the predicted price distribution for this customer if she had bid only on bank, two banks or all three of them by taking the minimum price from each combination of the quantile triplets.\n\n\n\nLoad packages and data\n\nlibrary(tidyverse)\noptions(scipen = 999, digits = 3)\n\n# data  bases\nload('mortgage_data/data_1_raw_database_v2.Rdata')\n\nWe have 3 data frames, one for each bank, each has 50 columns. The data sets been unified so all explanatory variables have the same meaning, which means, when we take an observations and predict its price on another bank’s modes the results make sense.\n\nls()\n\n[1] \"df_bank_A\" \"df_bank_B\" \"df_bank_C\"\n\nmap(ls(), ~ dim(get(.)))\n\n[[1]]\n[1] 25779    50\n\n[[2]]\n[1] 26958    50\n\n[[3]]\n[1] 44944    50\n\n\n\n\nRegression formula\nNext, an example of regression formula. In the research, we has nearly 50 explanatory variables including many categorical variables such as fixed effects for time, asset types and the loan portfolio. To run this big quantile regression one needs large data-set, otherwise the regression model will not converge, especially in the corner quantliles - for instance quantiles below 5 and above 95 may not converge. One solution would be to give up on the corner quantiles and to be satisfied with incomplete distribution. Another solution is to simply the regression formula. A third options could by to use some kind of Lasso Penalized Quantile Regression.\n\nreg_formula <-\nformula(\"interest ~ Loan_to_Value + Purchase_Purpose +\n  service_commission + asset_type + \n  log_loan_size + log_asset_value + log_disposable_income + \n  demographics +\n  porfolio_fixed_effects + \n  amortization_periods +\n  time_fixed_effect\")\n\n\n\nQuantile regressions\n\ntaus <- 1:99/100\nc(head(taus), \"....\", tail(taus))\n\n [1] \"0.01\" \"0.02\" \"0.03\" \"0.04\" \"0.05\" \"0.06\" \"....\" \"0.94\" \"0.95\" \"0.96\"\n[11] \"0.97\" \"0.98\" \"0.99\"\n\n\nRunning 297 quantile regression is an Embarrassingly parallel computing problem. Here we use the package furrr that implements parallel computing to the purrr map functions with future supported back-end. The change in syntax is just write future_map() instead of map() . The function quantreg::rq computes and creates the quantile regression model.\n\n\nlibrary(quantreg) # a quantile regression package authord by Roger Koenker himself\nlibrary(furrr)\n\nfuture::plan(multiprocess) # parallel processing\n\n\nmodels_A <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_A, method = 'pfn' ), .progress = T )\n\nmodels_B <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_B, method = 'pfn' ), .progress = T)\n\nmodels_C <- future_map(taus, ~ rq(reg_formula, tau = .x, data = df_bank_C, method = 'pfn' ), .progress = T)\n\n\nfuture:::ClusterRegistry(\"stop\") # close workers. \n\nFor each bank we get 99 quantile regressions packed in a list object.\n\n\n\nPredictions\nIn the mortgage research, we computed the simulation for each loan in the data. Then we reported summary statistics about the possible savings consumers could have had if they know or could bid more banks. For this example, a small data-set will suffice.\n\nload('mortgage_data/data_3_for_predictions.RData')\n\nFor each bank we run the predictions separately.\nThe next function computes the predictions for each list of models.\n\n# create predictions\nf_predict <- function(.models , .data, .bank){\n  # run pfedictions for each of the 9 models\n  x1 <- map_dfc(.models, ~ predict(.x, .data)) %>% rownames_to_column(var = \"id\") \n  \n  # arrange the data into long format to use with ggplot\n  x2 <- x1 %>% gather( key = \"quantile\", value = \"prediction\", 2:ncol(x))\n  x2 <- x2 %>% mutate(quantile = as.numeric(str_extract(quantile, \"\\\\d+\")),\n                      bank = .bank)\n  x2\n}\n\np_bank_A <- f_predict(models_A, df_for_predictions, \"A\")\np_bank_B <- f_predict(models_B, df_for_predictions, \"B\")\np_bank_C <- f_predict(models_C, df_for_predictions, \"C\")\n\n# bind  all predictions\np_banks <- bind_rows(p_bank_A, p_bank_B, p_bank_C) %>% mutate(id = as.numeric(id))\n\nLets see what we got:\n\nhead(p_banks %>% arrange(id))\n\n# A tibble: 6 x 4\n     id quantile prediction bank \n  <dbl>    <dbl>      <dbl> <chr>\n1     1        1       2.45 A    \n2     1        2       2.55 A    \n3     1        3       2.57 A    \n4     1        4       2.60 A    \n5     1        5       2.59 A    \n6     1        6       2.58 A    \n\n\nFor each loan we get 297 rows with each predicted price.\nLets look at the price distribution of a loan:\n\np_banks %>% filter(id == 10) %>%\n  ggplot(aes(x = quantile, y = prediction, color = bank)) +\n  geom_line()\n\n\n\n\nIts interesting to see that the price variance in bank B is very large. If this customer would be in the lower percentiles he is better of at bank B, but as the percentiles increase he is better of at bank A. the the end of the percentile range, the lowest price is in bank C.\n\n\nSimulation\nAs explained above in the algorithm section, its more interesting to assume that quentiles are not pre-determined but random. The next function draws random triplets of values \\(\\in [0, 99]\\) for chosen quantiles and matched them to the corresponding prices. Then, for each possible combination of those price values, the minimum price is chosen. For a given loan, 7 price distributions are constructed. 3 when she goes to only one bank - for each of the 3 banks, 3 more when she bids 2 banks: AB, AC, BC, and lastly when she bids all banks.\n\nf_simulation_dist_different_draw <- function(id_number, p_data, n = 1000) {\n  # draw random quantiles for each bank\n  r1 <- data.frame(\n    simulation_n = rep(1:n, 3),\n    id = id_number,\n    bank = c(rep(\"A\", n), rep(\"B\", n), rep(\"C\", n)), \n    quantile = round(runif(3 * n, 0.01, 0.99) * 100), stringsAsFactors = F )\n    \n # join predictions from the p_bank Rata \n  r2 <- left_join(r1, p_data ,by = c(\"id\", \"bank\", \"quantile\") )\n  \n  # For each draw triplet we choose all combinations of 2 banks,\n  # As if the customer got bids from two banks.\n  # There are 3 options: A-B, A-C, B-C\n  r3a <- r2 %>%  select(- quantile)\n  \n  # option 1\n  r3b1 <- r2 %>% filter(bank != \"C\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"A_B\")\n  \n  # option 2\n  r3b2 <- r2 %>% filter(bank != \"B\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"A_C\")\n  \n  # option 3\n  r3b3 <- r2 %>% filter(bank != \"C\") %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"B_C\")\n  \n  \n  # 1 option for bidding in all three banks\n  r3c <- r2 %>% group_by(simulation_n) %>%\n    mutate(prediction = min(prediction)) %>%\n    distinct(simulation_n, id, prediction) %>% ungroup() %>%\n    mutate(bank = \"all_3_banks\")\n  \n  # bind everything together\n  r4 <- bind_rows(r3a,r3b1, r3b2, r3b3, r3c)\n  \n  # a Dummy variable to choose from which bank the customer is\n  r4 <- r4 %>%\n    mutate(A = if_else( bank %in% c(\"A\", \"A_B\", \"A_C\", \"all_3_banks\"), 1, 0),\n           B = if_else( bank %in% c(\"B\", \"A_B\", \"B_C\", \"all_3_banks\"), 1, 0),\n           C = if_else( bank %in% c(\"C\", \"A_C\", \"B_C\", \"all_3_banks\"), 1, 0))\n  \n  r4\n}\n\nLets simulate the price distributions for loan 10:\n\ncustomer_10 <- f_simulation_dist_different_draw(id_number = 10,  p_banks)\n\nwe got a data ready to plot:\n\nhead(customer_10)\n\n  simulation_n id bank prediction A B C\n1            1 10    A       3.72 1 0 0\n2            2 10    A       3.18 1 0 0\n3            3 10    A       3.45 1 0 0\n4            4 10    A       4.41 1 0 0\n5            5 10    A       3.55 1 0 0\n6            6 10    A       4.02 1 0 0\n\n\nis not easy to look at 7 distributions simultaneously as well as it doesn’t really make sense. If the customer originally was from bank C, than we would like to see what she could have gaind from bidding banks A and B. The f_plot function plots 4 distributions from a certain bank costomer’s perspective.\n\nf_plot <- function(data, bank_choice){\n  data %>% filter(.data[[bank_choice]] == 1)  %>%\n    ggplot(aes(x = prediction, color = bank, fill = bank)) +\n    geom_density(alpha = 0.1) \n}\n\n\nf_plot(customer_10, 'C')\n\n\n\n\nwe can also look at it from every angle\n\np4 <- f_plot(customer_10, \"A\") + coord_cartesian(xlim = c(2.2, 3.6))\np5 <- f_plot(customer_10, \"B\")+ coord_cartesian(xlim = c(2.2, 3.6))\np6 <- f_plot(customer_10, \"C\")+ coord_cartesian(xlim = c(2.2, 3.6))\n\nlibrary(patchwork)\np4 / p5 / p6"
  },
  {
    "objectID": "Multinomial_Logit.html",
    "href": "Multinomial_Logit.html",
    "title": "Multinomial Logit Example",
    "section": "",
    "text": "In this document I’ll demonstrate merger simulation with a synthetic example. On the supply side assuming a differentiated Bertrand model and on the demand side assuming a multinomial logit demand function.\nThe Bertrand system of equations has 4 components:\n(1) Market shares.\n(2) Prices.\n(3) Derivatives of the demand function with respect to the prices.\n(4) Marginal costs of the firms."
  },
  {
    "objectID": "Multinomial_Logit.html#fixed-point-iteration",
    "href": "Multinomial_Logit.html#fixed-point-iteration",
    "title": "Multinomial Logit Example",
    "section": "Fixed point iteration",
    "text": "Fixed point iteration\nThis change in the merged firm incentives has a ripple effect on the entire market. The firm will raise its prices, some of the customers will leave its products and either buy it at the competitors or not buy it at all. To compute this, we need an iterative process that will end in a new equilibrium.\nAssuming convexity and continuity of the equation system, we can find a solution and know it is unique.\naccording to the model , the average utility from product j is: \\[ \\delta_j = x \\beta_j - \\alpha p + \\xi _j \\]\nFor each product we calculate the average utility.\nA change in price will change the utility from the product. This in turn, will be reflected in the demand function, the demand decreases when the price goes up and vice versa.\n\n# delta = x * b - alpha * p\n(delta <- log(s / s_0)) \n\n[1] -0.223  0.000  0.182\n\nexp(delta) / (1 + sum(exp(delta)))\n\n[1] 0.20 0.25 0.30"
  },
  {
    "objectID": "Multinomial_Logit.html#manual-iteration",
    "href": "Multinomial_Logit.html#manual-iteration",
    "title": "Multinomial Logit Example",
    "section": "Manual iteration",
    "text": "Manual iteration\nWe’ll compute the first few iterations be hand to see how it converges.\nIn every iteration 4 stages will take place:\n\nSolve the firms first order conditions subject to market shares to find a new price vector.\nCalculate the change in the price vector compared to the pre-merger state.\nfeed the new prices to consumer utility functions to get the change in utilities.\nGet the new market shares out from the demand function.\n\n\n# the price update procedure: \n# 1. solve FOC 1\n(p1 <- as.vector(mc + (1/ - alpha) * (1 / (1 -  theta_post %*% s))))\n\n[1] 55.7 79.8 80.0\n\n# 2. delta price\n(d_p <- (p1 - p))\n\n[1] 5.68 4.85 0.00\n\n# 3. delta in utility\n(d_delta <- d_p * alpha)\n\n[1] -0.568 -0.485  0.000\n\n# 4. solve demand for new utility\n(s1 <- exp(delta + d_delta) / (1 + sum(exp(delta + d_delta))))\n\n[1] 0.139 0.188 0.367\n\n\nGet the results and compere to the pre merger status.\n\nresults <-rbind(c(s, p), c(s1, p1)) \nresults\n\n      [,1]  [,2]  [,3] [,4] [,5] [,6]\n[1,] 0.200 0.250 0.300 50.0 75.0   80\n[2,] 0.139 0.188 0.367 55.7 79.8   80\n\n\nRepeat the process several more times.\n\n# solve FOC 2:\np2 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s1)))\nd_p <- (p2 - p) \nd_delta <- d_p * alpha\n# solve demand 2:\ns2 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s2, p2))\n# solve FOC 3:\np3 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s2)))\nd_p <- (p3 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns3 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s3, p3))\n# solve FOC 4:\np4 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s3)))\nd_p <- (p4 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns4 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s4, p4))\n# solve FOC 5:\np5 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s4)))\nd_p <- (p5 - p) \nd_delta <- d_p * alpha\n# solve demand 3:\ns5 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s5, p5))\n# solve FOC 6:\np6 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s5)))\nd_p <- (p6 - p) \nd_delta <- d_p * alpha\n# solve demand\ns6 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s6, p6))\n# solve FOC 7:\np7 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s6)))\nd_p <- (p7 - p) \nd_delta <- d_p * alpha\n# solve demand\ns7 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\nresults <- rbind(results, c(s7, p7))\n# solve FOC 8:\np8 <- as.vector(mc + (1/ -alpha) * (1/(1 -  theta_post %*% s7)))\nd_p <- (p8 - p) \nd_delta <- d_p * alpha\n# solve demand\ns8 <- exp(delta + d_delta) / (1 + sum(exp( delta + d_delta)))\n\nIn the results matrix, columns 1-3 are market shares and columns 4-6 are the prices.\nevery iteration the jumps in the values decreases.\n\nresults <- rbind(results, c(s8, p8))\ncolnames(results) <- c('s1', 's2', 's3', 'p1', 'p2', 'p3')\nresults\n\n         s1    s2    s3   p1   p2   p3\n [1,] 0.200 0.250 0.300 50.0 75.0 80.0\n [2,] 0.139 0.188 0.367 55.7 79.8 80.0\n [3,] 0.179 0.244 0.293 52.4 76.5 81.5\n [4,] 0.146 0.198 0.360 54.8 79.0 79.9\n [5,] 0.175 0.237 0.301 52.7 76.9 81.3\n [6,] 0.150 0.204 0.352 54.5 78.7 80.0\n [7,] 0.171 0.232 0.308 53.0 77.1 81.1\n [8,] 0.153 0.208 0.346 54.3 78.4 80.2\n [9,] 0.169 0.229 0.314 53.1 77.3 81.0\n\n\nWe can see where it’s going. the shares of products 1 and 2 are decreasing and product 3 is increasing. All prices are going up.\nLets calculate the new equilibrium."
  },
  {
    "objectID": "Multinomial_Logit.html#while-loop",
    "href": "Multinomial_Logit.html#while-loop",
    "title": "Multinomial Logit Example",
    "section": "While loop",
    "text": "While loop\nWe’ll limit the number of iterations with max_iter, and document the convergence process in a convergence_matrix .\n\nmax_iter <- 82 \ns_in <- s\ni <- 0\ns_delta_norm <- 1\nconvergence_matrix <- matrix(nrow = max_iter, ncol = length(s) * 2 + 2)\n\nwhile(s_delta_norm > 1e-6 & i < max_iter){\n  i <- i + 1\n  \n  # solve F.O.C\n  ( p_new <- as.vector(mc + (1/ - alpha ) * ( 1 / (1 -  theta_post %*% s_in) )))\n  \n  # change in utility\n  d_delta <- (p_new - p) * alpha\n  \n  # solve demand system\n  s_new <- exp(delta + d_delta) / (1 + sum(exp(delta + d_delta)))\n  \n  # norm of change in market shares\n  (s_delta_norm <- sqrt(sum((s_in - s_new) ^ 2)))\n  \n  # save resault for next iteration\n  s_in <- s_new\n  \n  convergence_matrix[i, ] <- c(s_new,  p_new, s_delta_norm, i)\n}\n\ncolnames(convergence_matrix) <- c(\"s1\", \"s2\", \"s3\", \"p1\", \"p2\", \"p3\", \"norm\", \"iteration\")\n\nconvergence_matrix\n\n         s1    s2    s3   p1   p2   p3        norm iteration\n [1,] 0.139 0.188 0.367 55.7 79.8 80.0 0.109827719         1\n [2,] 0.179 0.244 0.293 52.4 76.5 81.5 0.101188131         2\n [3,] 0.146 0.198 0.360 54.8 79.0 79.9 0.087867750         3\n [4,] 0.175 0.237 0.301 52.7 76.9 81.3 0.076132778         4\n [5,] 0.150 0.204 0.352 54.5 78.7 80.0 0.065389010         5\n [6,] 0.171 0.232 0.308 53.0 77.1 81.1 0.056301895         6\n [7,] 0.153 0.208 0.346 54.3 78.4 80.2 0.048324608         7\n [8,] 0.169 0.229 0.314 53.1 77.3 81.0 0.041574711         8\n [9,] 0.155 0.211 0.341 54.1 78.3 80.3 0.035693191         9\n[10,] 0.167 0.227 0.317 53.3 77.4 80.9 0.030698062        10\n[11,] 0.157 0.213 0.338 54.0 78.1 80.4 0.026361844        11\n[12,] 0.165 0.225 0.320 53.4 77.5 80.8 0.022667853        12\n[13,] 0.158 0.215 0.335 53.9 78.1 80.4 0.019469604        13\n[14,] 0.164 0.223 0.322 53.4 77.6 80.8 0.016738813        14\n[15,] 0.159 0.216 0.334 53.8 78.0 80.5 0.014379099        15\n[16,] 0.164 0.222 0.324 53.5 77.7 80.7 0.012360880        16\n[17,] 0.160 0.217 0.332 53.8 77.9 80.5 0.010619418        17\n[18,] 0.163 0.222 0.325 53.5 77.7 80.7 0.009128122        18\n[19,] 0.160 0.218 0.331 53.7 77.9 80.5 0.007842695        19\n[20,] 0.163 0.221 0.326 53.6 77.7 80.7 0.006740912        20\n[21,] 0.160 0.218 0.331 53.7 77.9 80.6 0.005791974        21\n[22,] 0.162 0.221 0.327 53.6 77.8 80.7 0.004978054        22\n[23,] 0.161 0.218 0.330 53.7 77.9 80.6 0.004277453        23\n[24,] 0.162 0.220 0.327 53.6 77.8 80.6 0.003676234        24\n[25,] 0.161 0.219 0.330 53.7 77.9 80.6 0.003158943        25\n[26,] 0.162 0.220 0.327 53.6 77.8 80.6 0.002714868        26\n[27,] 0.161 0.219 0.329 53.7 77.8 80.6 0.002332905        27\n[28,] 0.162 0.220 0.328 53.6 77.8 80.6 0.002004913        28\n[29,] 0.161 0.219 0.329 53.7 77.8 80.6 0.001722864        29\n[30,] 0.162 0.220 0.328 53.6 77.8 80.6 0.001480620        30\n[31,] 0.161 0.219 0.329 53.7 77.8 80.6 0.001272343        31\n[32,] 0.162 0.220 0.328 53.6 77.8 80.6 0.001093433        32\n[33,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000939630        33\n[34,] 0.162 0.220 0.328 53.6 77.8 80.6 0.000807498        34\n[35,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000693919        35\n[36,] 0.162 0.220 0.328 53.6 77.8 80.6 0.000596336        36\n[37,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000512461        37\n[38,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000440394        38\n[39,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000378453        39\n[40,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000325231        40\n[41,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000279488        41\n[42,] 0.162 0.219 0.328 53.6 77.8 80.6 0.000240183        42\n[43,] 0.161 0.219 0.329 53.7 77.8 80.6 0.000206402        43\n[44,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000177375        44\n[45,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000152428        45\n[46,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000130991        46\n[47,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000112568        47\n[48,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000096737        48\n[49,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000083132        49\n[50,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000071440        50\n[51,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000061393        51\n[52,] 0.161 0.219 0.328 53.6 77.8 80.6 0.000052759        52\n[53,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000045339        53\n[54,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000038962        54\n[55,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000033483        55\n[56,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000028774        56\n[57,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000024727        57\n[58,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000021249        58\n[59,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000018261        59\n[60,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000015693        60\n[61,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000013486        61\n[62,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000011589        62\n[63,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000009959        63\n[64,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000008559        64\n[65,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000007355        65\n[66,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000006320        66\n[67,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000005432        67\n[68,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000004668        68\n[69,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000004011        69\n[70,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000003447        70\n[71,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002962        71\n[72,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002546        72\n[73,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000002188        73\n[74,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001880        74\n[75,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001616        75\n[76,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001388        76\n[77,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001193        77\n[78,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000001025        78\n[79,] 0.161 0.219 0.328 53.7 77.8 80.6 0.000000881        79\n[80,]    NA    NA    NA   NA   NA   NA          NA        NA\n[81,]    NA    NA    NA   NA   NA   NA          NA        NA\n[82,]    NA    NA    NA   NA   NA   NA          NA        NA\n\n\nCompare the data before the merger with the prediction of the simulation about the merger effect on prices and market shares\n\n# attach preconditions with last row without NA's in the matrix\nfinal_resault <- \n  convergence_matrix[tail(which(rowSums(!is.na(convergence_matrix)) > 0), 1),]\n\n\nrbind(c(results[1,],norm = NA,iteration = 0),final_resault)\n\n                 s1    s2    s3   p1   p2   p3        norm iteration\n              0.200 0.250 0.300 50.0 75.0 80.0          NA         0\nfinal_resault 0.161 0.219 0.328 53.7 77.8 80.6 0.000000881        79\n\n\nThe market share of the outside option has increased:\n\nc(before = s_0, after = 1 - sum(final_resault[1:3]))\n\nbefore  after \n 0.250  0.291 \n\n\nPresent convergence process on a plot\n\npacman::p_load(tidyverse, patchwork)\n\np1 <- data.frame(convergence_matrix) %>%\n  select(iteration, p1, p2, p3) %>%\n  gather(k = \"k\", v = \"prices\", 2:4) %>%\n  ggplot(aes(x = iteration, y = prices, color = k)) + geom_line(size = 1.2)\n\np2 <- data.frame(convergence_matrix) %>%\n  select(iteration, s1, s2, s3) %>%\n  gather(k = \"k\", v = \"shares\", 2:4) %>%\n  ggplot(aes(x = iteration, y = shares, color = k)) + geom_line(size =1.2)\n\np1 / p2\n\n\n\n\nFirst order conditions:\n\ns_in + (theta_post*der) %*% (p_new - mc)\n\n         [,1]\n[1,] -0.01620\n[2,] -0.00271\n[3,]  0.01573\n\n\nThat’s it.\nIn the next example we’ll show a bit more complicated example of One level Nested logit. Also, the code will be warped up in functions for production."
  },
  {
    "objectID": "nested.html",
    "href": "nested.html",
    "title": "Nested Logit",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\noptions(scipen = 999, digits = 3)"
  },
  {
    "objectID": "nested.html#data-preparation",
    "href": "nested.html#data-preparation",
    "title": "Nested Logit",
    "section": "2.1. Data preparation",
    "text": "2.1. Data preparation\nPrepare data set - f1_init_calibration creates a new data frame in the global environment with necessary variables for the simulation.\nThe new data frame’s name is calibration data.\nWhen calling the function all arguments must be specified:\n(1) data frame that includes the columns listed next. (2) p: product prices. (3) q: quantities sold for each product. (4) firm: the firm that owns the product. (5) nest: the group of products the product belongs to according to the model specified the the researcher. (6) m_size: The market size assumed by the researcher. (The choice of market size actually determines the size of the outside option. This choice has a decisive effect on the results of the simulation, but this subject is beyond the scope of this document). (7) buyer: The acquirer firm. (8) seller: the purchased firm.\n\nf1_init_calibration <- function(data, p, q, firm, nest, m_size, buyer, seller){\n  \n  data <- data %>%\n    rename(p = {{p}}, q = {{q}},firm = {{firm}}, nest = {{nest}}, m_size = {{m_size}})\n  \n  nest_mat <<- outer(data$nest, data$nest, FUN = \"==\") * 1 # nest matrix\n  \n  calibration_data <<-\n    data %>%\n    select(p, q, firm, nest, m_size) %>%\n    mutate(\n      p = as.numeric(p),\n      q = as.numeric(q),\n      s = q / m_size,                   # product market share \n      s_0 = 1 - sum(s),                 # outside option market share\n      s_g = as.numeric(nest_mat %*% s), # share of each nest\n      s_jg = s / s_g,                   # share of product j in nest g\n      firm_post = if_else(firm == {{seller}}, {{buyer}}, firm),\n      n = n()) %>% na.omit()\n}"
  },
  {
    "objectID": "nested.html#derivative-matrix",
    "href": "nested.html#derivative-matrix",
    "title": "Nested Logit",
    "section": "2.2. Derivative matrix",
    "text": "2.2. Derivative matrix\nf2_derivatives builds and returns the derivative matrix from the shares, alpha and sigma.\nIn one level nested-logit there are 3 types of derivatives:\n(1) Own derivative (which should be all negative -> demand goes down as price goes up)\n(2) cross derivative for products in the same nest.\n(3) cross derivative for products in a different nest. (These are the same as the cross derivatives in the Multinomial Logit model)\n\nf2_derivatives <- function(shares, nest_shares, alpha, sigma){\n  \n  # preparations\n  s     <- shares\n  s_jg  <- nest_shares\n  n     <- length(s)\n  alpha <- - abs(alpha) # to avoid confusion. fix the sign of alpha\n  \n  # derivatives\n  cross_different  <- - alpha * s %o% s * (1 - nest_mat)\n  \n  cross_same <- \n    - alpha * (sigma / (1 - sigma) * s_jg + s) %o% s * nest_mat *\n    (1 - diag(x = 1,nrow = n))\n  \n  own <- alpha * s * (1 / (1-sigma) - sigma / (1-sigma) * s_jg - s)\n  \n  # warp it in the matrix\n  derivatives <- cross_different + cross_same + diag(own)\n  derivatives\n}"
  },
  {
    "objectID": "nested.html#pre-merger-conditions",
    "href": "nested.html#pre-merger-conditions",
    "title": "Nested Logit",
    "section": "2.3. Pre-merger conditions",
    "text": "2.3. Pre-merger conditions\nf3_pre_merger_conditions gets as inputs the calibration_data we created in the global environment, a derivatives matrix and the assumed \\(sigma\\), the parameter for correlation between products in the same nest.\nf3_pre_merger_conditions solves the first order condition of the equation system, and calculates several supplemented variables:\n(1) Profit margins and (2) Lerner index to check profitability, the (3) Delta which is the mean utility the consumer has with the prevailing prices and a (4) FOC condition to see if the iteration process that will come next converges to zero.\nThe new variables are added the the calibration_data in the global environment.\n\nf3_pre_merger_conditions <- function(c_data, derivatives, sigma){\n  \n  theta <- outer(c_data$firm, c_data$firm, FUN = \"==\") * 1 # ownership matrix\n  s <- c_data$s\n  p <- c_data$p\n\n  # system solution for marginal costs  \n  c_data$mc <- solve(derivatives * theta) %*% s + p \n  \n  c_data <- c_data %>%\n    mutate(margin = p - mc,\n           lerner_index = (p - mc) / p,\n           FOC = as.vector(s + (theta * derivatives) %*% (p - mc)),\n           delta =  log(s / s_0)  - sigma * log(s_jg)\n           )\n  \n  calibration_data <<- c_data\n}"
  },
  {
    "objectID": "nested.html#simulate-the-merger",
    "href": "nested.html#simulate-the-merger",
    "title": "Nested Logit",
    "section": "2.4. Simulate the merger",
    "text": "2.4. Simulate the merger\n\n2.4a. Nested logit Demand function\nIn the process of looking for a new equilibrium after the merger, we need to use the consumers demand function. Every time the prices change, the mean utility of the consumer changes, hence the quantity demand will change. f5_demand is the demand function. It calculates the market shares that will prevail given a vector of prices. Its arguments are delta- the mean utility from consumers have for each product, and \\(sigma\\) - the in-nest correlation parameter according to the nested logit model.\nThe user need not operate this function. It will be called from within f4_fixed_point that will calculate the new equilibrium.\n\nf5_demand <- function(delta, sigma){\n  \n  # demand function\n  exponent    <- exp(delta /(1-sigma))\n  D_g         <- unique(nest_mat, MARGIN = 1) %*%  exponent\n  denominator <- D_g ^ sigma * (1 + sum(D_g^(1-sigma)))\n  s_t         <- as.vector(exponent * unique(nest_mat, MARGIN = 2) %*% (1/ as.vector(denominator)))\n  \n  # nest shares\n  s_g  <- as.numeric(nest_mat %*% s_t)  # nest market share\n  s_jg <- s_t / s_g                     # share within the nest\n  \n  data.frame(s_t = s_t, s_jg_t = s_jg, delta_t = delta, n = length(s_t))\n}\n\n\n\n2.4b. Fixed point iteration\nf4_fixed_point calculates the new equilibrium with a fixed point iteration algorithm.\nIt uses the f5_demand f2_derivatives and solves the FOC iteratively until solution is reached or until the maximum number of iterations is reached.\nThe user has control over the maximum number of iterations and the tolarance desired for convergence.\nwe shall expand about the argument convergence_factor later.\n\nf4_fixed_point <- \n  function(data, alpha, sigma, max_iter = 100,\n           convergence_factor = 1, tolerance = 1e-3){\n    \n    theta_post <- outer(data$firm_post, data$firm_post, FUN = \"==\") * 1\n    s_in       <- data$s\n    p          <- data$p\n    mc         <- data$mc\n    delta      <- data$delta\n    der_new    <- f2_derivatives(calibration_data$s, calibration_data$s_jg, alpha, sigma)\n    log         <- matrix(nrow = max_iter, ncol = length(s_in) + 2)\n    i          <- 0\n    s_d_norm   <- 1\n    \n    while(i < max_iter & s_d_norm > tolerance){\n      \n      i         <- i + 1\n      p_new     <- mc - (solve(der_new * theta_post)) %*% s_in  # new price\n      delta_new <- delta - abs(alpha) * as.vector((p_new - p)/ convergence_factor)\n      s_new     <- f5_demand(delta_new, sigma)\n      der_new   <- f2_derivatives(s_new$s_t, s_new$s_jg_t, alpha, sigma)\n      s_d_norm  <- sqrt(sum((s_in - s_new[[1]]) ^ 2)) # measure convergence\n      s_in      <- s_new[[1]]    # new price vector to feed in\n      log[i,]    <- c(p_new, norm = s_d_norm,iteration = i) # results\n      \n    }\n    \n    log       <- data.frame(log) %>% drop_na()\n    names(log)<- c(paste(\"p\", 1:length(s_in), sep = \"_\"), \"norm\", \"iter\")\n    simulation_log <<- log\n    \n    tail     <- tail(log,1)\n    \n    data$new_prices <- tail %>% select(starts_with(\"p_\")) %>% t() %>% as.vector()\n    data$iterations <- tail %>% select(iter) %>% pull()\n    data$norm       <- tail %>% select(norm) %>% pull()\n    data$new_shares <- s_new[[1]]\n    \n    calibration_results <<- data %>%\n      select(firm, nest, p, new_prices, s, new_shares,\n             iterations, norm, q, m_size)\n    \n    \n  }"
  },
  {
    "objectID": "nested.html#build-toy-data",
    "href": "nested.html#build-toy-data",
    "title": "Nested Logit",
    "section": "3.1 Build toy data",
    "text": "3.1 Build toy data\nIts about time to see it all in action.\nwe’ll create a toy data of 6 products owned by 3 firms, divided into 2 nests.\nmarket size will be 100.\n\ndf <- data.frame(\n  company  = c(\"a\",     \"a\",     \"b\",    \"b\", \"c\", \"c\"),\n  nests    = c(\"pre\",   \"pre\",   \"pre\",   2,   2,   2),\n  quantity = c( 20,      5,       10,     5,   10,  25),\n  price    = c( 60,      40,      50,     45,  30,  30),\n  m_size   = 100,\n  not_needed_variable = \"junk\")\ndf\n\n  company nests quantity price m_size not_needed_variable\n1       a   pre       20    60    100                junk\n2       a   pre        5    40    100                junk\n3       b   pre       10    50    100                junk\n4       b     2        5    45    100                junk\n5       c     2       10    30    100                junk\n6       c     2       25    30    100                junk\n\n\nWe can prepare the data for simulation using f1_init_calibration:\nNote that all argument are needed for the function to know which variable is which:\nprices, quantities, firm, nest, market size, buyer and seller.\n\nf1_init_calibration(df, p = price, q = quantity, firm = company, nest = nests,\n                    m_size = m_size, buyer = 'a', seller = 'b')\n\nA new df namedcalibration_data was created in the global environment:\n\ncalibration_data\n\n   p  q firm nest m_size    s  s_0  s_g  s_jg firm_post n\n1 60 20    a  pre    100 0.20 0.25 0.35 0.571         a 6\n2 40  5    a  pre    100 0.05 0.25 0.35 0.143         a 6\n3 50 10    b  pre    100 0.10 0.25 0.35 0.286         a 6\n4 45  5    b    2    100 0.05 0.25 0.40 0.125         a 6\n5 30 10    c    2    100 0.10 0.25 0.40 0.250         c 6\n6 30 25    c    2    100 0.25 0.25 0.40 0.625         c 6"
  },
  {
    "objectID": "nested.html#assume-regression-results",
    "href": "nested.html#assume-regression-results",
    "title": "Nested Logit",
    "section": "3.2. Assume regression results",
    "text": "3.2. Assume regression results\nIf one does a Nested Logit demand estimation successfully , one has the right \\(sigma\\) and \\(alpha\\) to put into the simulation. here, we assume those parameters to be:\n\nsigma0.5 <- 0.5\nalpha0.1 <- 0.1\n\nThis does not have to be an arbitrary assumption, rather it can be based on the knowledge we have about the market."
  },
  {
    "objectID": "nested.html#checking-derivatives",
    "href": "nested.html#checking-derivatives",
    "title": "Nested Logit",
    "section": "3.3. Checking Derivatives",
    "text": "3.3. Checking Derivatives\nLets see how the derivative matrix is like for this toy data.\nIn actual simulation this function is being called from f4_fixed_point, so there is no need to call it.\n\nder <- f2_derivatives(calibration_data$s, calibration_data$s_jg, alpha0.1, sigma0.5)\nder\n\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n[1,] -0.02457  0.00386  0.00771  0.00100  0.00200  0.00500\n[2,]  0.00386 -0.00904  0.00193  0.00025  0.00050  0.00125\n[3,]  0.00771  0.00193 -0.01614  0.00050  0.00100  0.00250\n[4,]  0.00100  0.00025  0.00050 -0.00913  0.00175  0.00437\n[5,]  0.00200  0.00050  0.00100  0.00175 -0.01650  0.00875\n[6,]  0.00500  0.00125  0.00250  0.00438  0.00875 -0.02813\n\n\nGlimpsing on it, it look like the own derivative in the diagonal is negative and the rest is positive."
  },
  {
    "objectID": "nested.html#pre-merger-conditions-1",
    "href": "nested.html#pre-merger-conditions-1",
    "title": "Nested Logit",
    "section": "3.4. Pre-merger conditions",
    "text": "3.4. Pre-merger conditions\nNow we calculate the marginal costs of the firms. all new variables are added to the calibration_data .\n\nf3_pre_merger_conditions(calibration_data, der, sigma0.5)\ncalibration_data\n\n   p  q firm nest m_size    s  s_0  s_g  s_jg firm_post n   mc margin\n1 60 20    a  pre    100 0.20 0.25 0.35 0.571         a 6 50.3   9.66\n2 40  5    a  pre    100 0.05 0.25 0.35 0.143         a 6 30.3   9.66\n3 50 10    b  pre    100 0.10 0.25 0.35 0.286         a 6 43.6   6.38\n4 45  5    b    2    100 0.05 0.25 0.40 0.125         a 6 39.2   5.83\n5 30 10    c    2    100 0.10 0.25 0.40 0.250         c 6 17.1  12.90\n6 30 25    c    2    100 0.25 0.25 0.40 0.625         c 6 17.1  12.90\n  lerner_index                    FOC   delta\n1        0.161  0.0000000000000000000  0.0567\n2        0.241  0.0000000000000000000 -0.6365\n3        0.128 -0.0000000000000000278 -0.2899\n4        0.130  0.0000000000000000208 -0.5697\n5        0.430  0.0000000000000000000 -0.2231\n6        0.430  0.0000000000000000000  0.2350"
  },
  {
    "objectID": "nested.html#check-the-demand-system",
    "href": "nested.html#check-the-demand-system",
    "title": "Nested Logit",
    "section": "3.5. Check the demand system",
    "text": "3.5. Check the demand system\nFor the sake of presentation, lets see that the demand function works correctly.\nFeeding the delta’s consumers had, we should get the same market shares we assumed.\nEvery time the price will change, the delta will change and so the shares.\n\nf5_demand(calibration_data$delta, sigma0.5)\n\n   s_t s_jg_t delta_t n\n1 0.20  0.571  0.0567 6\n2 0.05  0.143 -0.6365 6\n3 0.10  0.286 -0.2899 6\n4 0.05  0.125 -0.5697 6\n5 0.10  0.250 -0.2231 6\n6 0.25  0.625  0.2350 6\n\n\nWe got the same market shares we chose when we first built our toy data, so its OK."
  },
  {
    "objectID": "nested.html#simulate-the-merger-1",
    "href": "nested.html#simulate-the-merger-1",
    "title": "Nested Logit",
    "section": "3.6. Simulate the merger",
    "text": "3.6. Simulate the merger\nAt last, we can simulate the merger.\nUsing the fixed point iteration, we get a new df named calibration_results with the results of the simulation.\n\nf4_fixed_point(calibration_data, alpha0.1, sigma0.5, convergence_factor = 1.2)\ncalibration_results\n\n  firm nest  p new_prices    s new_shares iterations     norm  q m_size\n1    a  pre 60       64.5 0.20     0.1736         89 0.000972 20    100\n2    a  pre 40       44.5 0.05     0.0434         89 0.000972  5    100\n3    b  pre 50       57.8 0.10     0.0502         89 0.000972 10    100\n4    b    2 45       46.7 0.05     0.0486         89 0.000972  5    100\n5    c    2 30       30.9 0.10     0.1109         89 0.000972 10    100\n6    c    2 30       30.9 0.25     0.2772         89 0.000972 25    100\n\n\ncalibration_results gives the new prices and new shares, reports how many iterations were needed to converge to the tolerance of 1/1000 and reports the norm of the change in prices in the last iteration.\nWe can watch the entire process in the simulation_log object created in the global environment.\n\nrbind(head(simulation_log),tail(simulation_log))\n\n    p_1  p_2  p_3  p_4  p_5  p_6     norm iter\n1  66.4 46.4 59.7 47.7 30.0 30.0 0.098581    1\n2  63.7 43.7 57.0 46.1 32.4 32.4 0.082170    2\n3  65.2 45.2 58.5 47.2 29.5 29.5 0.077062    3\n4  64.0 44.0 57.2 46.3 32.4 32.4 0.073149    4\n5  65.1 45.1 58.3 47.1 29.6 29.6 0.069658    5\n6  64.0 44.0 57.3 46.3 32.2 32.2 0.066111    6\n84 64.5 44.5 57.8 46.7 30.9 30.9 0.001254   84\n85 64.5 44.5 57.8 46.7 30.9 30.9 0.001191   85\n86 64.5 44.5 57.8 46.7 30.9 30.9 0.001132   86\n87 64.5 44.5 57.8 46.7 30.9 30.9 0.001076   87\n88 64.5 44.5 57.8 46.7 30.9 30.9 0.001023   88\n89 64.5 44.5 57.8 46.7 30.9 30.9 0.000972   89\n\nsimulation_log %>% select( - norm) %>%\n  pivot_longer(1:6) %>%\n  ggplot(aes(x = iter, y = value, color = name)) + geom_line()\n\n\n\n\nNote that when we called f4_fixed_point we used a convergenc_factor of 1.2.\nAs it happens, this data consists of only 6 observations and the fixed point algorithm doesn’t converge naturally with a convergence_factor of 1.\nAt best practice, one should use a convergence_factor of 1, and only if there’s a problem to choose a convergence_factor > 1.\nLets see what happens if we chose convergence_factor = 1.\n\nf4_fixed_point(calibration_data, alpha0.1, sigma0.5, convergence_factor = 1)\n\nPrices and shares are constantly jumping between two points.\n\nsimulation_log %>% select( - norm) %>%\n  pivot_longer(1:6) %>%\n  ggplot(aes(x = iter, y = value, color = name)) + geom_line()\n\n\n\n\ncalibration_resaults also tells us that the norm is far far away from zero:\n\ncalibration_results\n\n  firm nest  p new_prices    s new_shares iterations  norm  q m_size\n1    a  pre 60       60.9 0.20    0.28013        100 0.637 20    100\n2    a  pre 40       40.9 0.05    0.07003        100 0.637  5    100\n3    b  pre 50       54.2 0.10    0.07268        100 0.637 10    100\n4    b    2 45       44.5 0.05    0.20641        100 0.637  5    100\n5    c    2 30       52.8 0.10    0.00387        100 0.637 10    100\n6    c    2 30       52.8 0.25    0.00967        100 0.637 25    100"
  }
]